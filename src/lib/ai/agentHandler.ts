/**
 * AI Agent Handler for Engezna Smart Assistant
 *
 * This file handles the AI agent conversation loop using OpenAI with function calling.
 */

import OpenAI from 'openai'
import {
  AGENT_TOOLS,
  executeAgentTool,
  getAvailableTools,
  type ToolContext,
  type ToolResult
} from './agentTools'
import {
  buildSystemPrompt,
  type AgentContext,
  type AgentResponse,
  type ConversationTurn
} from './agentPrompt'

// =============================================================================
// TYPES
// =============================================================================

export interface AgentMessage {
  role: 'user' | 'assistant'
  content: string
}

export interface AgentStreamEvent {
  type: 'content' | 'tool_call' | 'tool_result' | 'done' | 'error'
  content?: string
  toolName?: string
  toolArgs?: Record<string, unknown>
  toolResult?: ToolResult
  error?: string
  response?: AgentResponse
}

export interface AgentHandlerOptions {
  context: AgentContext
  messages: AgentMessage[]
  onStream?: (event: AgentStreamEvent) => void
}

// =============================================================================
// OPENAI CLIENT (Lazy initialization)
// =============================================================================

let openaiClient: OpenAI | null = null

function getOpenAIClient(): OpenAI {
  if (!openaiClient) {
    if (!process.env.OPENAI_API_KEY) {
      throw new Error('OPENAI_API_KEY environment variable is not set')
    }
    openaiClient = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY
    })
  }
  return openaiClient
}

// =============================================================================
// CONVERT TOOLS TO OPENAI FORMAT
// =============================================================================

function convertToolsToOpenAI(context: ToolContext): OpenAI.Chat.Completions.ChatCompletionTool[] {
  const availableTools = getAvailableTools(context)

  return availableTools.map(tool => ({
    type: 'function' as const,
    function: {
      name: tool.name,
      description: tool.description,
      parameters: tool.parameters
    }
  }))
}

// =============================================================================
// MAIN AGENT HANDLER
// =============================================================================

export async function runAgent(options: AgentHandlerOptions): Promise<AgentResponse> {
  const { context, messages, onStream } = options

  // Build system prompt
  const systemPrompt = buildSystemPrompt(context)

  // Convert tools to OpenAI format
  const tools = convertToolsToOpenAI(context)

  // Build messages array for OpenAI
  const openaiMessages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
    { role: 'system', content: systemPrompt },
    ...messages.map(msg => ({
      role: msg.role as 'user' | 'assistant',
      content: msg.content
    }))
  ]

  // Track conversation turns for this request
  const turns: ConversationTurn[] = []
  let finalResponse: AgentResponse = { content: '' }

  // Run the agent loop (max 5 iterations to prevent infinite loops)
  for (let iteration = 0; iteration < 5; iteration++) {
    try {
      // Call OpenAI with optimized settings for natural conversation
      const completion = await getOpenAIClient().chat.completions.create({
        model: 'gpt-4o-mini', // Can upgrade to 'gpt-4o' for better conversation quality
        messages: openaiMessages,
        tools: tools.length > 0 ? tools : undefined,
        tool_choice: tools.length > 0 ? 'auto' : undefined,
        temperature: 0.85, // Higher for more natural, varied responses
        max_tokens: 1500,  // More room for detailed, helpful responses
        presence_penalty: 0.1, // Slight penalty to reduce repetition
        frequency_penalty: 0.1 // Encourage diverse vocabulary
      })

      const choice = completion.choices[0]
      const message = choice.message

      // Check if the model wants to call tools
      if (message.tool_calls && message.tool_calls.length > 0) {
        // Add assistant message with tool calls to history
        openaiMessages.push({
          role: 'assistant',
          content: message.content || '',
          tool_calls: message.tool_calls
        })

        // Execute each tool call
        for (const toolCall of message.tool_calls) {
          // Handle different tool call types
          if (toolCall.type !== 'function') continue
          const toolName = toolCall.function.name
          const toolArgs = JSON.parse(toolCall.function.arguments)

          // Stream tool call event
          onStream?.({
            type: 'tool_call',
            toolName,
            toolArgs
          })

          // Execute the tool
          const result = await executeAgentTool(toolName, toolArgs, context)

          // Stream tool result event
          onStream?.({
            type: 'tool_result',
            toolName,
            toolResult: result
          })

          // Add tool result to conversation
          turns.push({
            role: 'tool',
            content: JSON.stringify(result),
            toolName,
            toolResult: result,
            timestamp: new Date()
          })

          // Add tool result to OpenAI messages
          openaiMessages.push({
            role: 'tool',
            tool_call_id: toolCall.id,
            content: JSON.stringify(result)
          })
        }

        // Continue the loop to get the final response
        continue
      }

      // No tool calls - this is the final response
      const content = message.content || ''

      // Stream content
      onStream?.({
        type: 'content',
        content
      })

      // Parse the response to extract structured data
      finalResponse = parseAgentOutput(content, turns, context.providerId || context.cartProviderId)

      // Stream done event
      onStream?.({
        type: 'done',
        response: finalResponse
      })

      break

    } catch (error) {
      console.error('[Agent Error]:', error)

      // Never expose technical errors to users - just show a friendly message
      // Log the actual error for debugging but give user a positive experience

      finalResponse = {
        content: 'Ù…Ø´ Ù„Ø§Ù‚ÙŠ Ù†ØªØ§Ø¦Ø¬ Ø¯Ù„ÙˆÙ‚ØªÙŠ. Ø¬Ø±Ø¨ ØªØ§Ù†ÙŠ Ø£Ùˆ Ø§Ø³Ø£Ù„Ù†ÙŠ Ø³Ø¤Ø§Ù„ ØªØ§Ù†ÙŠ ğŸ˜Š',
        suggestions: ['ğŸ”„ Ø¬Ø±Ø¨ ØªØ§Ù†ÙŠ', 'ğŸ½ï¸ Ø§Ù„Ù…Ù†ÙŠÙˆ', 'ğŸ  Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©']
      }

      onStream?.({
        type: 'done',
        response: finalResponse
      })

      break
    }
  }

  return finalResponse
}

// =============================================================================
// STREAMING AGENT HANDLER
// =============================================================================

export async function* runAgentStream(options: AgentHandlerOptions): AsyncGenerator<AgentStreamEvent> {
  const { context, messages } = options

  // Build system prompt
  const systemPrompt = buildSystemPrompt(context)

  // Convert tools to OpenAI format
  const tools = convertToolsToOpenAI(context)

  // Build messages array for OpenAI
  const openaiMessages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
    { role: 'system', content: systemPrompt },
    ...messages.map(msg => ({
      role: msg.role as 'user' | 'assistant',
      content: msg.content
    }))
  ]

  const turns: ConversationTurn[] = []

  // Run the agent loop
  for (let iteration = 0; iteration < 5; iteration++) {
    try {
      // Streaming with optimized settings for natural conversation
      const stream = await getOpenAIClient().chat.completions.create({
        model: 'gpt-4o-mini', // Can upgrade to 'gpt-4o' for better conversation quality
        messages: openaiMessages,
        tools: tools.length > 0 ? tools : undefined,
        tool_choice: tools.length > 0 ? 'auto' : undefined,
        temperature: 0.85, // Higher for more natural, varied responses
        max_tokens: 1500,  // More room for detailed, helpful responses
        presence_penalty: 0.1, // Slight penalty to reduce repetition
        frequency_penalty: 0.1, // Encourage diverse vocabulary
        stream: true
      })

      let accumulatedContent = ''
      const toolCalls: Array<{
        id: string
        name: string
        arguments: string
      }> = []

      for await (const chunk of stream) {
        const delta = chunk.choices[0]?.delta

        // Handle content streaming
        if (delta?.content) {
          accumulatedContent += delta.content
          yield {
            type: 'content',
            content: delta.content
          }
        }

        // Handle tool calls
        if (delta?.tool_calls) {
          for (const toolCallDelta of delta.tool_calls) {
            if (toolCallDelta.index !== undefined) {
              if (!toolCalls[toolCallDelta.index]) {
                toolCalls[toolCallDelta.index] = {
                  id: toolCallDelta.id || '',
                  name: toolCallDelta.function?.name || '',
                  arguments: ''
                }
              }

              if (toolCallDelta.id) {
                toolCalls[toolCallDelta.index].id = toolCallDelta.id
              }
              if (toolCallDelta.function?.name) {
                toolCalls[toolCallDelta.index].name = toolCallDelta.function.name
              }
              if (toolCallDelta.function?.arguments) {
                toolCalls[toolCallDelta.index].arguments += toolCallDelta.function.arguments
              }
            }
          }
        }
      }

      // Process tool calls if any
      if (toolCalls.length > 0) {
        // Add assistant message with tool calls
        openaiMessages.push({
          role: 'assistant',
          content: accumulatedContent || null,
          tool_calls: toolCalls.map(tc => ({
            id: tc.id,
            type: 'function' as const,
            function: {
              name: tc.name,
              arguments: tc.arguments
            }
          }))
        })

        // Execute each tool
        for (const toolCall of toolCalls) {
          const toolArgs = JSON.parse(toolCall.arguments)

          yield {
            type: 'tool_call',
            toolName: toolCall.name,
            toolArgs
          }

          const result = await executeAgentTool(toolCall.name, toolArgs, context)

          yield {
            type: 'tool_result',
            toolName: toolCall.name,
            toolResult: result
          }

          turns.push({
            role: 'tool',
            content: JSON.stringify(result),
            toolName: toolCall.name,
            toolResult: result,
            timestamp: new Date()
          })

          openaiMessages.push({
            role: 'tool',
            tool_call_id: toolCall.id,
            content: JSON.stringify(result)
          })
        }

        // Continue loop
        continue
      }

      // Final response
      const finalResponse = parseAgentOutput(accumulatedContent, turns, context.providerId || context.cartProviderId)

      yield {
        type: 'done',
        response: finalResponse
      }

      return

    } catch (error) {
      console.error('[Agent Stream Error]:', error)

      // Never expose technical errors to users - just show a friendly message
      yield {
        type: 'done',
        response: {
          content: 'Ù…Ø´ Ù„Ø§Ù‚ÙŠ Ù†ØªØ§Ø¦Ø¬ Ø¯Ù„ÙˆÙ‚ØªÙŠ. Ø¬Ø±Ø¨ ØªØ§Ù†ÙŠ Ø£Ùˆ Ø§Ø³Ø£Ù„Ù†ÙŠ Ø³Ø¤Ø§Ù„ ØªØ§Ù†ÙŠ ğŸ˜Š',
          suggestions: ['ğŸ”„ Ø¬Ø±Ø¨ ØªØ§Ù†ÙŠ', 'ğŸ½ï¸ Ø§Ù„Ù…Ù†ÙŠÙˆ', 'ğŸ  Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©']
        }
      }

      return
    }
  }

  // Max iterations reached
  yield {
    type: 'done',
    response: {
      content: 'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù…Ø´ Ù‚Ø§Ø¯Ø± Ø£ÙƒÙ…Ù„ Ø§Ù„Ø·Ù„Ø¨ Ø¯Ù„ÙˆÙ‚ØªÙŠ. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© ØªØ§Ù†ÙŠØ©.',
      suggestions: ['ğŸ”„ Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© ØªØ§Ù†ÙŠØ©']
    }
  }
}

// =============================================================================
// HELPER FUNCTIONS
// =============================================================================

/**
 * Generate dynamic quick replies based on context
 */
function generateDynamicQuickReplies(
  content: string,
  hasCartAction: boolean,
  hasProducts: boolean,
  productId?: string,
  toolsUsed?: string[],
  providerId?: string
): { suggestions: string[]; quickReplies: AgentResponse['quickReplies'] } {

  // Helper: create menu navigation payload
  const menuPayload = providerId
    ? `navigate:/ar/providers/${providerId}`
    : 'ÙˆØ±Ù‘ÙŠÙ†ÙŠ Ø§Ù„Ù…Ù†ÙŠÙˆ'

  // After adding to cart
  if (hasCartAction) {
    return {
      suggestions: ['ğŸ›’ Ø´ÙˆÙ Ø§Ù„Ø³Ù„Ø©', 'â• Ø£Ø¶Ù Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ©', 'âœ… ÙƒÙ…Ù„ Ù„Ù„Ø¯ÙØ¹'],
      quickReplies: [
        { title: 'ğŸ›’ Ø´ÙˆÙ Ø§Ù„Ø³Ù„Ø©', payload: 'Ø§ÙŠÙ‡ ÙÙŠ Ø§Ù„Ø³Ù„Ø©ØŸ' },
        { title: 'â• Ø£Ø¶Ù Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ©', payload: 'Ø¹Ø§ÙŠØ² Ø£Ø¶ÙŠÙ Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ©' },
        { title: 'âœ… ÙƒÙ…Ù„ Ù„Ù„Ø¯ÙØ¹', payload: 'navigate:/ar/checkout' }
      ]
    }
  }

  // After search with products found
  if (hasProducts && productId) {
    return {
      suggestions: ['âœ… Ø¶ÙŠÙ Ù„Ù„Ø³Ù„Ø©', 'ğŸ“‹ ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØªØ±', 'ğŸ” Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ©'],
      quickReplies: [
        { title: 'âœ… Ø¶ÙŠÙ Ù„Ù„Ø³Ù„Ø©', payload: 'Ø¶ÙŠÙ Ø§Ù„Ø£ÙˆÙ„ Ù„Ù„Ø³Ù„Ø©' },
        { title: 'ğŸ“‹ ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØªØ±', payload: 'Ø¹Ø§ÙŠØ² ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØªØ±' },
        { title: 'ğŸ” Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ©', payload: 'Ø¹Ø§ÙŠØ² Ø£Ø¨Ø­Ø« Ø¹Ù† Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ©' }
      ]
    }
  }

  // Order-related context
  if (content.includes('Ø·Ù„Ø¨') && (content.includes('ØªØªØ¨Ø¹') || content.includes('Ø­Ø§Ù„Ø©'))) {
    return {
      suggestions: ['ğŸ“ ØªØªØ¨Ø¹ Ø§Ù„Ø·Ù„Ø¨', 'ğŸ“ Ø§ØªØµÙ„ Ø¨Ø§Ù„Ù…Ø·Ø¹Ù…', 'âŒ Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø·Ù„Ø¨'],
      quickReplies: [
        { title: 'ğŸ“ ØªØªØ¨Ø¹ Ø§Ù„Ø·Ù„Ø¨', payload: 'ÙÙŠÙ† Ø·Ù„Ø¨ÙŠ Ø¯Ù„ÙˆÙ‚ØªÙŠØŸ' },
        { title: 'ğŸ“ Ø§ØªØµÙ„ Ø¨Ø§Ù„Ù…Ø·Ø¹Ù…', payload: 'Ø¹Ø§ÙŠØ² Ø±Ù‚Ù… Ø§Ù„Ù…Ø·Ø¹Ù…' },
        { title: 'âŒ Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø·Ù„Ø¨', payload: 'Ø¹Ø§ÙŠØ² Ø£Ù„ØºÙŠ Ø§Ù„Ø·Ù„Ø¨' }
      ]
    }
  }

  // Complaint or problem context
  if (content.includes('Ù…Ø´ÙƒÙ„Ø©') || content.includes('Ø´ÙƒÙˆÙ‰') || content.includes('Ø²Ø¹Ù„Ø§Ù†')) {
    return {
      suggestions: ['ğŸ“ ÙƒÙ„Ù… Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡', 'ğŸ“ Ø§ÙƒØªØ¨ Ø´ÙƒÙˆÙ‰', 'ğŸ”™ Ø±Ø¬ÙˆØ¹'],
      quickReplies: [
        { title: 'ğŸ“ ÙƒÙ„Ù… Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡', payload: 'Ø¹Ø§ÙŠØ² Ø£ÙƒÙ„Ù… Ø­Ø¯ Ù…Ù† Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡' },
        { title: 'ğŸ“ Ø§ÙƒØªØ¨ Ø´ÙƒÙˆÙ‰', payload: 'Ø¹Ø§ÙŠØ² Ø£Ø¹Ù…Ù„ Ø´ÙƒÙˆÙ‰ Ø±Ø³Ù…ÙŠØ©' },
        { title: 'ğŸ”™ Ø±Ø¬ÙˆØ¹', payload: 'Ø®Ù„Ø§Øµ Ù…Ø´ Ù…Ø­ØªØ§Ø¬' }
      ]
    }
  }

  // Cart summary context
  if (content.includes('Ø§Ù„Ø³Ù„Ø©') && (content.includes('ÙÙŠÙ‡Ø§') || content.includes('Ø¥Ø¬Ù…Ø§Ù„ÙŠ'))) {
    return {
      suggestions: ['âœ… ÙƒÙ…Ù„ Ù„Ù„Ø¯ÙØ¹', 'â• Ø£Ø¶Ù Ø­Ø§Ø¬Ø©', 'ğŸ—‘ï¸ ÙØ¶ÙŠ Ø§Ù„Ø³Ù„Ø©'],
      quickReplies: [
        { title: 'âœ… ÙƒÙ…Ù„ Ù„Ù„Ø¯ÙØ¹', payload: 'navigate:/ar/checkout' },
        { title: 'â• Ø£Ø¶Ù Ø­Ø§Ø¬Ø©', payload: 'Ø¹Ø§ÙŠØ² Ø£Ø¶ÙŠÙ Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ©' },
        { title: 'ğŸ—‘ï¸ ÙØ¶ÙŠ Ø§Ù„Ø³Ù„Ø©', payload: 'Ø§Ù…Ø³Ø­ Ø§Ù„Ø³Ù„Ø© ÙƒÙ„Ù‡Ø§' }
      ]
    }
  }

  // Delivery info context
  if (content.includes('ØªÙˆØµÙŠÙ„') || content.includes('Ø±Ø³ÙˆÙ…')) {
    return {
      suggestions: ['âœ… ØªÙ…Ø§Ù…ØŒ Ø§Ø·Ù„Ø¨', 'ğŸ” Ø¨Ø­Ø« ØªØ§Ù†ÙŠ', 'ğŸ“‹ Ø§Ù„Ù…Ù†ÙŠÙˆ'],
      quickReplies: [
        { title: 'âœ… ØªÙ…Ø§Ù…ØŒ Ø§Ø·Ù„Ø¨', payload: 'Ø¹Ø§ÙŠØ² Ø£Ø·Ù„Ø¨' },
        { title: 'ğŸ” Ø¨Ø­Ø« ØªØ§Ù†ÙŠ', payload: 'Ø¹Ø§ÙŠØ² Ø£Ø¨Ø­Ø« Ø¹Ù† Ø­Ø§Ø¬Ø©' },
        { title: 'ğŸ“‹ Ø§Ù„Ù…Ù†ÙŠÙˆ', payload: menuPayload }
      ]
    }
  }

  // Menu/categories context
  if (toolsUsed?.includes('get_provider_categories') || toolsUsed?.includes('get_menu_items')) {
    return {
      suggestions: ['ğŸ• Ø¨ÙŠØªØ²Ø§', 'ğŸ” Ø¨Ø±Ø¬Ø±', 'ğŸ¥— Ø³Ù„Ø·Ø§Øª', 'ğŸ” Ø¨Ø­Ø«'],
      quickReplies: [
        { title: 'ğŸ• Ø¨ÙŠØªØ²Ø§', payload: 'Ø¹Ø§ÙŠØ² Ø¨ÙŠØªØ²Ø§' },
        { title: 'ğŸ” Ø¨Ø±Ø¬Ø±', payload: 'Ø¹Ø§ÙŠØ² Ø¨Ø±Ø¬Ø±' },
        { title: 'ğŸ¥— Ø³Ù„Ø·Ø§Øª', payload: 'Ø¹Ø§ÙŠØ² Ø³Ù„Ø·Ø©' },
        { title: 'ğŸ” Ø¨Ø­Ø«', payload: 'Ø¹Ø§ÙŠØ² Ø£Ø¨Ø­Ø« Ø¹Ù† Ø­Ø§Ø¬Ø© Ù…Ø¹ÙŠÙ†Ø©' }
      ]
    }
  }

  // Greeting/welcome context
  if (content.includes('Ø£Ù‡Ù„Ø§Ù‹') || content.includes('ØµØ¨Ø§Ø­') || content.includes('Ù…Ø³Ø§Ø¡')) {
    return {
      suggestions: ['ğŸ” Ø¹Ø§ÙŠØ² Ø¢ÙƒÙ„', 'ğŸ“¦ Ø·Ù„Ø¨Ø§ØªÙŠ', 'ğŸ”¥ Ø§Ù„Ø¹Ø±ÙˆØ¶'],
      quickReplies: [
        { title: 'ğŸ” Ø¹Ø§ÙŠØ² Ø¢ÙƒÙ„', payload: 'Ø¹Ø§ÙŠØ² Ø£Ø·Ù„Ø¨ Ø£ÙƒÙ„' },
        { title: 'ğŸ“¦ Ø·Ù„Ø¨Ø§ØªÙŠ', payload: 'ÙÙŠÙ† Ø·Ù„Ø¨Ø§ØªÙŠØŸ' },
        { title: 'ğŸ”¥ Ø§Ù„Ø¹Ø±ÙˆØ¶', payload: 'ÙÙŠÙ‡ Ø¹Ø±ÙˆØ¶ Ø§ÙŠÙ‡ØŸ' }
      ]
    }
  }

  // Default suggestions
  return {
    suggestions: ['ğŸ½ï¸ Ø´ÙˆÙ Ø§Ù„Ù…Ù†ÙŠÙˆ', 'ğŸ”¥ Ø§Ù„Ø¹Ø±ÙˆØ¶', 'ğŸ“¦ Ø·Ù„Ø¨Ø§ØªÙŠ'],
    quickReplies: [
      { title: 'ğŸ½ï¸ Ø´ÙˆÙ Ø§Ù„Ù…Ù†ÙŠÙˆ', payload: menuPayload },
      { title: 'ğŸ”¥ Ø§Ù„Ø¹Ø±ÙˆØ¶', payload: 'ÙÙŠÙ‡ Ø¹Ø±ÙˆØ¶ Ø§ÙŠÙ‡ØŸ' },
      { title: 'ğŸ“¦ Ø·Ù„Ø¨Ø§ØªÙŠ', payload: 'ÙÙŠÙ† Ø·Ù„Ø¨Ø§ØªÙŠØŸ' }
    ]
  }
}

/**
 * Parse agent output to extract structured response
 */
function parseAgentOutput(content: string, turns: ConversationTurn[], providerId?: string): AgentResponse {
  const response: AgentResponse = {
    content: content.trim(),
    suggestions: [],
    quickReplies: [],
    products: []
  }

  // Track which tools were used
  const toolsUsed: string[] = []

  // Extract products and cart actions from tool results
  for (const turn of turns) {
    if (turn.role === 'tool' && turn.toolResult) {
      if (turn.toolName) {
        toolsUsed.push(turn.toolName)
      }

      const result = turn.toolResult as ToolResult
      if (result.success && result.data) {
        const data = result.data as Record<string, unknown>

        // Check for cart_action (from add_to_cart tool)
        if (data.cart_action) {
          response.cartAction = data.cart_action as AgentResponse['cartAction']
        }

        // Check if it's an array of menu items
        if (Array.isArray(result.data)) {
          const items = result.data as Array<Record<string, unknown>>
          if (items.length > 0 && items[0].name_ar && items[0].price) {
            response.products = items.slice(0, 5).map(item => ({
              id: item.id as string,
              name: item.name_ar as string,
              price: item.price as number,
              image: item.image_url as string | undefined,
              hasVariants: item.has_variants as boolean | undefined,
              providerId: item.provider_id as string | undefined,
              providerName: (item.providers as { name_ar?: string })?.name_ar
            }))
          }
        }
      }
    }
  }

  // Generate dynamic quick replies based on context
  // Use provider ID from first product if available, otherwise fall back to context
  const effectiveProviderId = response.products?.[0]?.providerId || providerId

  const { suggestions, quickReplies } = generateDynamicQuickReplies(
    content,
    !!response.cartAction,
    !!(response.products && response.products.length > 0),
    response.products?.[0]?.id,
    toolsUsed,
    effectiveProviderId
  )

  response.suggestions = suggestions
  response.quickReplies = quickReplies

  return response
}

/**
 * Simple agent for quick responses (no streaming)
 */
export async function quickAgentResponse(
  userMessage: string,
  context: AgentContext
): Promise<AgentResponse> {
  return runAgent({
    context,
    messages: [{ role: 'user', content: userMessage }]
  })
}
