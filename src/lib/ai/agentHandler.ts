/**
 * AI Agent Handler for Engezna Smart Assistant
 *
 * This file handles the AI agent conversation loop using OpenAI with function calling.
 */

import OpenAI from 'openai'
import {
  AGENT_TOOLS,
  executeAgentTool,
  getAvailableTools,
  type ToolContext,
  type ToolResult
} from './agentTools'
import { validateToolParams, checkRateLimit } from './toolValidation'
import {
  buildSystemPrompt,
  type AgentContext,
  type AgentResponse,
  type ConversationTurn
} from './agentPrompt'

// =============================================================================
// TYPES
// =============================================================================

export interface AgentMessage {
  role: 'user' | 'assistant'
  content: string
}

export interface AgentStreamEvent {
  type: 'content' | 'tool_call' | 'tool_result' | 'done' | 'error'
  content?: string
  toolName?: string
  toolArgs?: Record<string, unknown>
  toolResult?: ToolResult
  error?: string
  response?: AgentResponse
}

export interface AgentHandlerOptions {
  context: AgentContext
  messages: AgentMessage[]
  onStream?: (event: AgentStreamEvent) => void
}

// =============================================================================
// OPENAI CLIENT (Lazy initialization)
// =============================================================================

let openaiClient: OpenAI | null = null

function getOpenAIClient(): OpenAI {
  if (!openaiClient) {
    if (!process.env.OPENAI_API_KEY) {
      throw new Error('OPENAI_API_KEY environment variable is not set')
    }
    openaiClient = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY
    })
  }
  return openaiClient
}

// =============================================================================
// CONVERT TOOLS TO OPENAI FORMAT
// =============================================================================

function convertToolsToOpenAI(context: ToolContext): OpenAI.Chat.Completions.ChatCompletionTool[] {
  const availableTools = getAvailableTools(context)

  return availableTools.map(tool => ({
    type: 'function' as const,
    function: {
      name: tool.name,
      description: tool.description,
      parameters: tool.parameters
    }
  }))
}

// =============================================================================
// MAIN AGENT HANDLER
// =============================================================================

export async function runAgent(options: AgentHandlerOptions): Promise<AgentResponse> {
  const { context, messages, onStream } = options

  // Build system prompt
  const systemPrompt = buildSystemPrompt(context)

  // Convert tools to OpenAI format
  const tools = convertToolsToOpenAI(context)

  // Build messages array for OpenAI
  const openaiMessages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
    { role: 'system', content: systemPrompt },
    ...messages.map(msg => ({
      role: msg.role as 'user' | 'assistant',
      content: msg.content
    }))
  ]

  // Track conversation turns for this request
  const turns: ConversationTurn[] = []
  let finalResponse: AgentResponse = { content: '' }

  // Run the agent loop (max 5 iterations to prevent infinite loops)
  for (let iteration = 0; iteration < 5; iteration++) {
    try {
      // Call OpenAI with optimized settings for natural conversation
      const completion = await getOpenAIClient().chat.completions.create({
        model: 'gpt-4o-mini', // Can upgrade to 'gpt-4o' for better conversation quality
        messages: openaiMessages,
        tools: tools.length > 0 ? tools : undefined,
        tool_choice: tools.length > 0 ? 'auto' : undefined,
        temperature: 0.85, // Higher for more natural, varied responses
        max_tokens: 1500,  // More room for detailed, helpful responses
        presence_penalty: 0.1, // Slight penalty to reduce repetition
        frequency_penalty: 0.1 // Encourage diverse vocabulary
      })

      const choice = completion.choices[0]
      const message = choice.message

      // Check if the model wants to call tools
      if (message.tool_calls && message.tool_calls.length > 0) {
        // Add assistant message with tool calls to history
        openaiMessages.push({
          role: 'assistant',
          content: message.content || '',
          tool_calls: message.tool_calls
        })

        // Execute each tool call
        for (const toolCall of message.tool_calls) {
          // Handle different tool call types
          if (toolCall.type !== 'function') continue
          const toolName = toolCall.function.name
          const toolArgs = JSON.parse(toolCall.function.arguments)

          // Stream tool call event
          onStream?.({
            type: 'tool_call',
            toolName,
            toolArgs
          })

          // Validate tool parameters before execution
          const validation = validateToolParams(toolName, toolArgs, context)
          if (!validation.valid) {
            // Return validation error as tool result
            const validationResult: ToolResult = {
              success: false,
              error: validation.error,
              message: validation.message
            }

            onStream?.({
              type: 'tool_result',
              toolName,
              toolResult: validationResult
            })

            turns.push({
              role: 'tool',
              content: JSON.stringify(validationResult),
              toolName,
              toolResult: validationResult,
              timestamp: new Date()
            })

            openaiMessages.push({
              role: 'tool',
              tool_call_id: toolCall.id,
              content: JSON.stringify(validationResult)
            })

            continue
          }

          // Check rate limits (using a simple conversation identifier)
          const conversationId = context.customerId || 'anonymous'
          const rateLimit = checkRateLimit(toolName, conversationId)
          if (!rateLimit.allowed) {
            const rateLimitResult: ToolResult = {
              success: false,
              error: 'rate_limited',
              message: rateLimit.message
            }

            onStream?.({
              type: 'tool_result',
              toolName,
              toolResult: rateLimitResult
            })

            turns.push({
              role: 'tool',
              content: JSON.stringify(rateLimitResult),
              toolName,
              toolResult: rateLimitResult,
              timestamp: new Date()
            })

            openaiMessages.push({
              role: 'tool',
              tool_call_id: toolCall.id,
              content: JSON.stringify(rateLimitResult)
            })

            continue
          }

          // Execute the tool
          const result = await executeAgentTool(toolName, toolArgs, context)

          // Stream tool result event
          onStream?.({
            type: 'tool_result',
            toolName,
            toolResult: result
          })

          // Add tool result to conversation
          turns.push({
            role: 'tool',
            content: JSON.stringify(result),
            toolName,
            toolResult: result,
            timestamp: new Date()
          })

          // Add tool result to OpenAI messages
          openaiMessages.push({
            role: 'tool',
            tool_call_id: toolCall.id,
            content: JSON.stringify(result)
          })
        }

        // Continue the loop to get the final response
        continue
      }

      // No tool calls - this is the final response
      const content = message.content || ''

      // Stream content
      onStream?.({
        type: 'content',
        content
      })

      // Parse the response to extract structured data
      finalResponse = parseAgentOutput(content, turns, context.providerId || context.cartProviderId)

      // Stream done event
      onStream?.({
        type: 'done',
        response: finalResponse
      })

      break

    } catch (error) {
      console.error('[Agent Error]:', error)

      // Never expose technical errors to users - just show a friendly message
      // Log the actual error for debugging but give user a positive experience

      finalResponse = {
        content: 'ŸÖÿ¥ ŸÑÿßŸÇŸä ŸÜÿ™ÿßÿ¶ÿ¨ ÿØŸÑŸàŸÇÿ™Ÿä. ÿ¨ÿ±ÿ® ÿ™ÿßŸÜŸä ÿ£Ÿà ÿßÿ≥ÿ£ŸÑŸÜŸä ÿ≥ÿ§ÿßŸÑ ÿ™ÿßŸÜŸä üòä',
        suggestions: ['üîÑ ÿ¨ÿ±ÿ® ÿ™ÿßŸÜŸä', 'üçΩÔ∏è ÿßŸÑŸÖŸÜŸäŸà', 'üè† ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ©']
      }

      onStream?.({
        type: 'done',
        response: finalResponse
      })

      break
    }
  }

  return finalResponse
}

// =============================================================================
// STREAMING AGENT HANDLER
// =============================================================================

export async function* runAgentStream(options: AgentHandlerOptions): AsyncGenerator<AgentStreamEvent> {
  const { context, messages } = options

  // Build system prompt
  const systemPrompt = buildSystemPrompt(context)

  // Convert tools to OpenAI format
  const tools = convertToolsToOpenAI(context)

  // Build messages array for OpenAI
  const openaiMessages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
    { role: 'system', content: systemPrompt },
    ...messages.map(msg => ({
      role: msg.role as 'user' | 'assistant',
      content: msg.content
    }))
  ]

  const turns: ConversationTurn[] = []

  // Run the agent loop
  for (let iteration = 0; iteration < 5; iteration++) {
    try {
      // Streaming with optimized settings for natural conversation
      const stream = await getOpenAIClient().chat.completions.create({
        model: 'gpt-4o-mini', // Can upgrade to 'gpt-4o' for better conversation quality
        messages: openaiMessages,
        tools: tools.length > 0 ? tools : undefined,
        tool_choice: tools.length > 0 ? 'auto' : undefined,
        temperature: 0.85, // Higher for more natural, varied responses
        max_tokens: 1500,  // More room for detailed, helpful responses
        presence_penalty: 0.1, // Slight penalty to reduce repetition
        frequency_penalty: 0.1, // Encourage diverse vocabulary
        stream: true
      })

      let accumulatedContent = ''
      const toolCalls: Array<{
        id: string
        name: string
        arguments: string
      }> = []

      for await (const chunk of stream) {
        const delta = chunk.choices[0]?.delta

        // Handle content streaming
        if (delta?.content) {
          accumulatedContent += delta.content
          yield {
            type: 'content',
            content: delta.content
          }
        }

        // Handle tool calls
        if (delta?.tool_calls) {
          for (const toolCallDelta of delta.tool_calls) {
            if (toolCallDelta.index !== undefined) {
              if (!toolCalls[toolCallDelta.index]) {
                toolCalls[toolCallDelta.index] = {
                  id: toolCallDelta.id || '',
                  name: toolCallDelta.function?.name || '',
                  arguments: ''
                }
              }

              if (toolCallDelta.id) {
                toolCalls[toolCallDelta.index].id = toolCallDelta.id
              }
              if (toolCallDelta.function?.name) {
                toolCalls[toolCallDelta.index].name = toolCallDelta.function.name
              }
              if (toolCallDelta.function?.arguments) {
                toolCalls[toolCallDelta.index].arguments += toolCallDelta.function.arguments
              }
            }
          }
        }
      }

      // Process tool calls if any
      if (toolCalls.length > 0) {
        // Add assistant message with tool calls
        openaiMessages.push({
          role: 'assistant',
          content: accumulatedContent || null,
          tool_calls: toolCalls.map(tc => ({
            id: tc.id,
            type: 'function' as const,
            function: {
              name: tc.name,
              arguments: tc.arguments
            }
          }))
        })

        // Execute each tool
        for (const toolCall of toolCalls) {
          const toolArgs = JSON.parse(toolCall.arguments)
          const toolName = toolCall.name

          yield {
            type: 'tool_call',
            toolName,
            toolArgs
          }

          // Validate tool parameters before execution
          const validation = validateToolParams(toolName, toolArgs, context)
          if (!validation.valid) {
            const validationResult: ToolResult = {
              success: false,
              error: validation.error,
              message: validation.message
            }

            yield {
              type: 'tool_result',
              toolName,
              toolResult: validationResult
            }

            turns.push({
              role: 'tool',
              content: JSON.stringify(validationResult),
              toolName,
              toolResult: validationResult,
              timestamp: new Date()
            })

            openaiMessages.push({
              role: 'tool',
              tool_call_id: toolCall.id,
              content: JSON.stringify(validationResult)
            })

            continue
          }

          // Check rate limits
          const conversationId = context.customerId || 'anonymous'
          const rateLimit = checkRateLimit(toolName, conversationId)
          if (!rateLimit.allowed) {
            const rateLimitResult: ToolResult = {
              success: false,
              error: 'rate_limited',
              message: rateLimit.message
            }

            yield {
              type: 'tool_result',
              toolName,
              toolResult: rateLimitResult
            }

            turns.push({
              role: 'tool',
              content: JSON.stringify(rateLimitResult),
              toolName,
              toolResult: rateLimitResult,
              timestamp: new Date()
            })

            openaiMessages.push({
              role: 'tool',
              tool_call_id: toolCall.id,
              content: JSON.stringify(rateLimitResult)
            })

            continue
          }

          const result = await executeAgentTool(toolName, toolArgs, context)

          yield {
            type: 'tool_result',
            toolName: toolCall.name,
            toolResult: result
          }

          turns.push({
            role: 'tool',
            content: JSON.stringify(result),
            toolName: toolCall.name,
            toolResult: result,
            timestamp: new Date()
          })

          openaiMessages.push({
            role: 'tool',
            tool_call_id: toolCall.id,
            content: JSON.stringify(result)
          })
        }

        // Continue loop
        continue
      }

      // Final response
      const finalResponse = parseAgentOutput(accumulatedContent, turns, context.providerId || context.cartProviderId)

      yield {
        type: 'done',
        response: finalResponse
      }

      return

    } catch (error) {
      console.error('[Agent Stream Error]:', error)

      // Never expose technical errors to users - just show a friendly message
      yield {
        type: 'done',
        response: {
          content: 'ŸÖÿ¥ ŸÑÿßŸÇŸä ŸÜÿ™ÿßÿ¶ÿ¨ ÿØŸÑŸàŸÇÿ™Ÿä. ÿ¨ÿ±ÿ® ÿ™ÿßŸÜŸä ÿ£Ÿà ÿßÿ≥ÿ£ŸÑŸÜŸä ÿ≥ÿ§ÿßŸÑ ÿ™ÿßŸÜŸä üòä',
          suggestions: ['üîÑ ÿ¨ÿ±ÿ® ÿ™ÿßŸÜŸä', 'üçΩÔ∏è ÿßŸÑŸÖŸÜŸäŸà', 'üè† ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ©']
        }
      }

      return
    }
  }

  // Max iterations reached
  yield {
    type: 'done',
    response: {
      content: 'ÿπÿ∞ÿ±ÿßŸãÿå ŸÖÿ¥ ŸÇÿßÿØÿ± ÿ£ŸÉŸÖŸÑ ÿßŸÑÿ∑ŸÑÿ® ÿØŸÑŸàŸÇÿ™Ÿä. ÿ≠ÿßŸàŸÑ ŸÖÿ±ÿ© ÿ™ÿßŸÜŸäÿ©.',
      suggestions: ['üîÑ ÿ≠ÿßŸàŸÑ ŸÖÿ±ÿ© ÿ™ÿßŸÜŸäÿ©']
    }
  }
}

// =============================================================================
// HELPER FUNCTIONS
// =============================================================================

/**
 * Smart Quick Reply Generator
 *
 * Analyzes the AI response content and tool results to generate
 * contextually relevant quick replies. More intelligent than hardcoded logic.
 */
function generateDynamicQuickReplies(
  content: string,
  hasCartAction: boolean,
  hasProducts: boolean,
  productId?: string,
  toolsUsed?: string[],
  providerId?: string
): { suggestions: string[]; quickReplies: AgentResponse['quickReplies'] } {

  // Helper: create menu navigation payload
  const menuPayload = providerId
    ? `navigate:/ar/providers/${providerId}`
    : 'Ÿàÿ±ŸëŸäŸÜŸä ÿßŸÑŸÖŸÜŸäŸà'

  // Analyze content for intent signals
  const contentLower = content.toLowerCase()

  // =================================================================
  // INTENT DETECTION: Analyze what the AI said to determine best actions
  // =================================================================

  // Check if AI is asking about size/variant selection
  const isAskingVariant = contentLower.includes('ÿ≠ÿ¨ŸÖ') ||
    contentLower.includes('ÿ£Ÿä ÿ≠ÿ¨ŸÖ') ||
    contentLower.includes('ÿµÿ∫Ÿäÿ±') && contentLower.includes('ŸÉÿ®Ÿäÿ±') ||
    contentLower.includes('ÿßÿÆÿ™ÿßÿ±')

  // Check if AI is asking about quantity
  const isAskingQuantity = contentLower.includes('ŸÉÿßŸÖ Ÿàÿßÿ≠ÿØ') ||
    contentLower.includes('ŸÉÿßŸÖ Ÿàÿßÿ≠ÿØÿ©') ||
    contentLower.includes('ÿßŸÑŸÉŸÖŸäÿ©')

  // Check if AI is confirming something
  const isConfirming = contentLower.includes('ÿµÿ≠ÿü') ||
    contentLower.includes('ÿµÿ≠ ŸÉÿØŸá') ||
    contentLower.includes('ÿ™ŸÖÿßŸÖ ŸÉÿØŸá')

  // Check if search returned no results
  const noResults = contentLower.includes('ŸÖÿ¥ ŸÑÿßŸÇŸä') ||
    contentLower.includes('ŸÖŸÑŸÇÿ™ÿ¥') ||
    contentLower.includes('ŸÖŸÅŸäÿ¥')

  // Check if AI is showing promotions
  const showingPromotions = toolsUsed?.includes('get_promotions') ||
    contentLower.includes('ÿπÿ±ÿ∂') || contentLower.includes('ÿÆÿµŸÖ')

  // =================================================================
  // CONTEXTUAL QUICK REPLIES
  // =================================================================

  // Size/Variant selection needed
  if (isAskingVariant && hasProducts) {
    return {
      suggestions: ['ÿµÿ∫Ÿäÿ±', 'Ÿàÿ≥ÿ∑', 'ŸÉÿ®Ÿäÿ±'],
      quickReplies: [
        { title: 'üìè ÿµÿ∫Ÿäÿ±', payload: 'ÿπÿßŸäÿ≤ ÿßŸÑÿ≠ÿ¨ŸÖ ÿßŸÑÿµÿ∫Ÿäÿ±' },
        { title: 'üìè Ÿàÿ≥ÿ∑', payload: 'ÿπÿßŸäÿ≤ ÿßŸÑÿ≠ÿ¨ŸÖ ÿßŸÑŸàÿ≥ÿ∑' },
        { title: 'üìè ŸÉÿ®Ÿäÿ±', payload: 'ÿπÿßŸäÿ≤ ÿßŸÑÿ≠ÿ¨ŸÖ ÿßŸÑŸÉÿ®Ÿäÿ±' }
      ]
    }
  }

  // Quantity selection needed
  if (isAskingQuantity) {
    return {
      suggestions: ['1Ô∏è‚É£ Ÿàÿßÿ≠ÿØÿ©', '2Ô∏è‚É£ ÿßÿ™ŸÜŸäŸÜ', '3Ô∏è‚É£ ÿ™ŸÑÿßÿ™ÿ©'],
      quickReplies: [
        { title: '1Ô∏è‚É£ Ÿàÿßÿ≠ÿØÿ©', payload: 'Ÿàÿßÿ≠ÿØÿ© ÿ®ÿ≥' },
        { title: '2Ô∏è‚É£ ÿßÿ™ŸÜŸäŸÜ', payload: 'ÿßÿ™ŸÜŸäŸÜ' },
        { title: '3Ô∏è‚É£ ÿ™ŸÑÿßÿ™ÿ©', payload: 'ÿ™ŸÑÿßÿ™ÿ©' }
      ]
    }
  }

  // Confirmation needed
  if (isConfirming) {
    return {
      suggestions: ['‚úÖ ÿ£ŸäŸàŸá ÿ™ŸÖÿßŸÖ', '‚ùå ŸÑÿ£ ÿ∫Ÿäÿ±', 'üîÑ ÿπÿØŸÑ ÿßŸÑŸÉŸÖŸäÿ©'],
      quickReplies: [
        { title: '‚úÖ ÿ£ŸäŸàŸá ÿ™ŸÖÿßŸÖ', payload: 'ÿ£ŸäŸàŸá ÿ∂ŸäŸÅ ŸÑŸÑÿ≥ŸÑÿ©' },
        { title: '‚ùå ŸÑÿ£ ÿ∫Ÿäÿ±', payload: 'ŸÑÿ£ ÿπÿßŸäÿ≤ ÿ£ÿ∫Ÿäÿ±' },
        { title: 'üîÑ ÿπÿØŸÑ ÿßŸÑŸÉŸÖŸäÿ©', payload: 'ÿπÿßŸäÿ≤ ÿ£ÿ∫Ÿäÿ± ÿßŸÑŸÉŸÖŸäÿ©' }
      ]
    }
  }

  // After adding to cart
  if (hasCartAction) {
    return {
      suggestions: ['üõí ÿ¥ŸàŸÅ ÿßŸÑÿ≥ŸÑÿ©', '‚ûï ÿ£ÿ∂ŸÅ ÿ≠ÿßÿ¨ÿ© ÿ™ÿßŸÜŸäÿ©', '‚úÖ ŸÉŸÖŸÑ ŸÑŸÑÿØŸÅÿπ'],
      quickReplies: [
        { title: 'üõí ÿ¥ŸàŸÅ ÿßŸÑÿ≥ŸÑÿ©', payload: 'ÿßŸäŸá ŸÅŸä ÿßŸÑÿ≥ŸÑÿ©ÿü' },
        { title: '‚ûï ÿ£ÿ∂ŸÅ ÿ≠ÿßÿ¨ÿ© ÿ™ÿßŸÜŸäÿ©', payload: 'ÿπÿßŸäÿ≤ ÿ£ÿ∂ŸäŸÅ ÿ≠ÿßÿ¨ÿ© ÿ™ÿßŸÜŸäÿ©' },
        { title: '‚úÖ ŸÉŸÖŸÑ ŸÑŸÑÿØŸÅÿπ', payload: 'navigate:/ar/checkout' }
      ]
    }
  }

  // No results found - help user search differently
  if (noResults) {
    return {
      suggestions: ['üîç ÿ®ÿ≠ÿ´ ÿ™ÿßŸÜŸä', 'üìã ÿ¥ŸàŸÅ ÿßŸÑŸÖŸÜŸäŸà', 'üî• ÿßŸÑÿπÿ±Ÿàÿ∂'],
      quickReplies: [
        { title: 'üîç ÿ®ÿ≠ÿ´ ÿ™ÿßŸÜŸä', payload: 'ÿπÿßŸäÿ≤ ÿ£ÿ®ÿ≠ÿ´ ÿπŸÜ ÿ≠ÿßÿ¨ÿ© ÿ™ÿßŸÜŸäÿ©' },
        { title: 'üìã ÿ¥ŸàŸÅ ÿßŸÑŸÖŸÜŸäŸà', payload: menuPayload },
        { title: 'üî• ÿßŸÑÿπÿ±Ÿàÿ∂', payload: 'ŸÅŸäŸá ÿπÿ±Ÿàÿ∂ ÿßŸäŸá ÿØŸÑŸàŸÇÿ™Ÿäÿü' }
      ]
    }
  }

  // After search with products found
  if (hasProducts && productId) {
    return {
      suggestions: ['‚úÖ ÿ∂ŸäŸÅ ŸÑŸÑÿ≥ŸÑÿ©', 'üìã ÿ™ŸÅÿßÿµŸäŸÑ ÿ£ŸÉÿ™ÿ±', 'üîç ÿ≠ÿßÿ¨ÿ© ÿ™ÿßŸÜŸäÿ©'],
      quickReplies: [
        { title: '‚úÖ ÿ∂ŸäŸÅ ŸÑŸÑÿ≥ŸÑÿ©', payload: 'ÿ∂ŸäŸÅ ÿßŸÑÿ£ŸàŸÑ ŸÑŸÑÿ≥ŸÑÿ©' },
        { title: 'üìã ÿ™ŸÅÿßÿµŸäŸÑ ÿ£ŸÉÿ™ÿ±', payload: 'ÿπÿßŸäÿ≤ ÿ™ŸÅÿßÿµŸäŸÑ ÿ£ŸÉÿ™ÿ±' },
        { title: 'üîç ÿ≠ÿßÿ¨ÿ© ÿ™ÿßŸÜŸäÿ©', payload: 'ÿπÿßŸäÿ≤ ÿ£ÿ®ÿ≠ÿ´ ÿπŸÜ ÿ≠ÿßÿ¨ÿ© ÿ™ÿßŸÜŸäÿ©' }
      ]
    }
  }

  // Showing promotions
  if (showingPromotions) {
    return {
      suggestions: ['üéÅ ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑÿπÿ±ÿ∂', 'üçΩÔ∏è ÿ¥ŸàŸÅ ÿßŸÑŸÖŸÜŸäŸà', 'üîç ÿ®ÿ≠ÿ´'],
      quickReplies: [
        { title: 'üéÅ ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑÿπÿ±ÿ∂', payload: 'ÿπÿßŸäÿ≤ ÿ£ÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑÿπÿ±ÿ∂ ÿØŸá' },
        { title: 'üçΩÔ∏è ÿ¥ŸàŸÅ ÿßŸÑŸÖŸÜŸäŸà', payload: menuPayload },
        { title: 'üîç ÿ®ÿ≠ÿ´', payload: 'ÿπÿßŸäÿ≤ ÿ£ÿ®ÿ≠ÿ´ ÿπŸÜ ÿ≠ÿßÿ¨ÿ©' }
      ]
    }
  }

  // Order tracking context
  if (toolsUsed?.includes('track_order') || toolsUsed?.includes('get_order_status')) {
    return {
      suggestions: ['üìç ÿ™ÿ™ÿ®ÿπ ÿßŸÑÿ∑ŸÑÿ®', 'üìû ÿßÿ™ÿµŸÑ ÿ®ÿßŸÑŸÖÿ∑ÿπŸÖ', '‚ùå ÿ•ŸÑÿ∫ÿßÿ° ÿßŸÑÿ∑ŸÑÿ®'],
      quickReplies: [
        { title: 'üìç ÿ™ÿ™ÿ®ÿπ ÿßŸÑÿ∑ŸÑÿ®', payload: 'ŸÅŸäŸÜ ÿ∑ŸÑÿ®Ÿä ÿØŸÑŸàŸÇÿ™Ÿäÿü' },
        { title: 'üìû ÿßÿ™ÿµŸÑ ÿ®ÿßŸÑŸÖÿ∑ÿπŸÖ', payload: 'ÿπÿßŸäÿ≤ ÿ±ŸÇŸÖ ÿßŸÑŸÖÿ∑ÿπŸÖ' },
        { title: '‚ùå ÿ•ŸÑÿ∫ÿßÿ° ÿßŸÑÿ∑ŸÑÿ®', payload: 'ÿπÿßŸäÿ≤ ÿ£ŸÑÿ∫Ÿä ÿßŸÑÿ∑ŸÑÿ®' }
      ]
    }
  }

  // Complaint or problem context
  if (contentLower.includes('ŸÖÿ¥ŸÉŸÑÿ©') || contentLower.includes('ÿ¥ŸÉŸàŸâ') ||
      contentLower.includes('ÿ≤ÿπŸÑÿßŸÜ') || contentLower.includes('ŸÖÿπŸÑÿ¥')) {
    return {
      suggestions: ['üìû ŸÉŸÑŸÖ ÿÆÿØŸÖÿ© ÿßŸÑÿπŸÖŸÑÿßÿ°', 'üìù ÿßŸÉÿ™ÿ® ÿ¥ŸÉŸàŸâ', 'üîô ÿ±ÿ¨Ÿàÿπ'],
      quickReplies: [
        { title: 'üìû ŸÉŸÑŸÖ ÿÆÿØŸÖÿ© ÿßŸÑÿπŸÖŸÑÿßÿ°', payload: 'ÿπÿßŸäÿ≤ ÿ£ŸÉŸÑŸÖ ÿ≠ÿØ ŸÖŸÜ ÿÆÿØŸÖÿ© ÿßŸÑÿπŸÖŸÑÿßÿ°' },
        { title: 'üìù ÿßŸÉÿ™ÿ® ÿ¥ŸÉŸàŸâ', payload: 'ÿπÿßŸäÿ≤ ÿ£ÿπŸÖŸÑ ÿ¥ŸÉŸàŸâ ÿ±ÿ≥ŸÖŸäÿ©' },
        { title: 'üîô ÿ±ÿ¨Ÿàÿπ', payload: 'ÿÆŸÑÿßÿµ ŸÖÿ¥ ŸÖÿ≠ÿ™ÿßÿ¨' }
      ]
    }
  }

  // Cart summary context
  if (toolsUsed?.includes('get_cart_summary') ||
      (contentLower.includes('ÿßŸÑÿ≥ŸÑÿ©') && contentLower.includes('ŸÅŸäŸáÿß'))) {
    return {
      suggestions: ['‚úÖ ŸÉŸÖŸÑ ŸÑŸÑÿØŸÅÿπ', '‚ûï ÿ£ÿ∂ŸÅ ÿ≠ÿßÿ¨ÿ©', 'üóëÔ∏è ŸÅÿ∂Ÿä ÿßŸÑÿ≥ŸÑÿ©'],
      quickReplies: [
        { title: '‚úÖ ŸÉŸÖŸÑ ŸÑŸÑÿØŸÅÿπ', payload: 'navigate:/ar/checkout' },
        { title: '‚ûï ÿ£ÿ∂ŸÅ ÿ≠ÿßÿ¨ÿ©', payload: 'ÿπÿßŸäÿ≤ ÿ£ÿ∂ŸäŸÅ ÿ≠ÿßÿ¨ÿ© ÿ™ÿßŸÜŸäÿ©' },
        { title: 'üóëÔ∏è ŸÅÿ∂Ÿä ÿßŸÑÿ≥ŸÑÿ©', payload: 'ÿßŸÖÿ≥ÿ≠ ÿßŸÑÿ≥ŸÑÿ© ŸÉŸÑŸáÿß' }
      ]
    }
  }

  // Delivery info context
  if (toolsUsed?.includes('get_delivery_info') ||
      contentLower.includes('ÿ™ŸàÿµŸäŸÑ') || contentLower.includes('ÿ±ÿ≥ŸàŸÖ')) {
    return {
      suggestions: ['‚úÖ ÿ™ŸÖÿßŸÖÿå ÿßÿ∑ŸÑÿ®', 'üîç ÿ®ÿ≠ÿ´ ÿ™ÿßŸÜŸä', 'üìã ÿßŸÑŸÖŸÜŸäŸà'],
      quickReplies: [
        { title: '‚úÖ ÿ™ŸÖÿßŸÖÿå ÿßÿ∑ŸÑÿ®', payload: 'ÿπÿßŸäÿ≤ ÿ£ÿ∑ŸÑÿ®' },
        { title: 'üîç ÿ®ÿ≠ÿ´ ÿ™ÿßŸÜŸä', payload: 'ÿπÿßŸäÿ≤ ÿ£ÿ®ÿ≠ÿ´ ÿπŸÜ ÿ≠ÿßÿ¨ÿ©' },
        { title: 'üìã ÿßŸÑŸÖŸÜŸäŸà', payload: menuPayload }
      ]
    }
  }

  // Menu/categories context
  if (toolsUsed?.includes('get_provider_categories') || toolsUsed?.includes('get_menu_items')) {
    return {
      suggestions: ['üçï ÿ®Ÿäÿ™ÿ≤ÿß', 'üçî ÿ®ÿ±ÿ¨ÿ±', 'ü•ó ÿ≥ŸÑÿ∑ÿßÿ™', 'üîç ÿ®ÿ≠ÿ´'],
      quickReplies: [
        { title: 'üçï ÿ®Ÿäÿ™ÿ≤ÿß', payload: 'ÿπÿßŸäÿ≤ ÿ®Ÿäÿ™ÿ≤ÿß' },
        { title: 'üçî ÿ®ÿ±ÿ¨ÿ±', payload: 'ÿπÿßŸäÿ≤ ÿ®ÿ±ÿ¨ÿ±' },
        { title: 'ü•ó ÿ≥ŸÑÿ∑ÿßÿ™', payload: 'ÿπÿßŸäÿ≤ ÿ≥ŸÑÿ∑ÿ©' },
        { title: 'üîç ÿ®ÿ≠ÿ´', payload: 'ÿπÿßŸäÿ≤ ÿ£ÿ®ÿ≠ÿ´ ÿπŸÜ ÿ≠ÿßÿ¨ÿ© ŸÖÿπŸäŸÜÿ©' }
      ]
    }
  }

  // Greeting/welcome context
  if (contentLower.includes('ÿ£ŸáŸÑÿßŸã') || contentLower.includes('ÿ£ŸáŸÑÿß') ||
      contentLower.includes('ÿµÿ®ÿßÿ≠') || contentLower.includes('ŸÖÿ≥ÿßÿ°')) {
    return {
      suggestions: ['üçî ÿπÿßŸäÿ≤ ÿ¢ŸÉŸÑ', 'üì¶ ÿ∑ŸÑÿ®ÿßÿ™Ÿä', 'üî• ÿßŸÑÿπÿ±Ÿàÿ∂'],
      quickReplies: [
        { title: 'üçî ÿπÿßŸäÿ≤ ÿ¢ŸÉŸÑ', payload: 'ÿπÿßŸäÿ≤ ÿ£ÿ∑ŸÑÿ® ÿ£ŸÉŸÑ' },
        { title: 'üì¶ ÿ∑ŸÑÿ®ÿßÿ™Ÿä', payload: 'ŸÅŸäŸÜ ÿ∑ŸÑÿ®ÿßÿ™Ÿäÿü' },
        { title: 'üî• ÿßŸÑÿπÿ±Ÿàÿ∂', payload: 'ŸÅŸäŸá ÿπÿ±Ÿàÿ∂ ÿßŸäŸáÿü' }
      ]
    }
  }

  // Default suggestions
  return {
    suggestions: ['üçΩÔ∏è ÿ¥ŸàŸÅ ÿßŸÑŸÖŸÜŸäŸà', 'üî• ÿßŸÑÿπÿ±Ÿàÿ∂', 'üì¶ ÿ∑ŸÑÿ®ÿßÿ™Ÿä'],
    quickReplies: [
      { title: 'üçΩÔ∏è ÿ¥ŸàŸÅ ÿßŸÑŸÖŸÜŸäŸà', payload: menuPayload },
      { title: 'üî• ÿßŸÑÿπÿ±Ÿàÿ∂', payload: 'ŸÅŸäŸá ÿπÿ±Ÿàÿ∂ ÿßŸäŸáÿü' },
      { title: 'üì¶ ÿ∑ŸÑÿ®ÿßÿ™Ÿä', payload: 'ŸÅŸäŸÜ ÿ∑ŸÑÿ®ÿßÿ™Ÿäÿü' }
    ]
  }
}

/**
 * Sanitize AI response to remove unsafe content
 * This is a POST-PROCESSING GUARDRAIL to ensure no URLs or markdown images slip through
 */
function sanitizeAgentResponse(content: string): string {
  let sanitized = content

  // Remove markdown image syntax: ![alt](url)
  sanitized = sanitized.replace(/!\[([^\]]*)\]\([^)]+\)/g, '')

  // Remove markdown links but keep the text: [text](url) -> text
  sanitized = sanitized.replace(/\[([^\]]+)\]\([^)]+\)/g, '$1')

  // Remove raw URLs (http, https, ftp)
  sanitized = sanitized.replace(/https?:\/\/[^\s<>"\)]+/gi, '')
  sanitized = sanitized.replace(/ftp:\/\/[^\s<>"\)]+/gi, '')

  // Remove any remaining URL-like patterns
  sanitized = sanitized.replace(/www\.[^\s<>"\)]+/gi, '')

  // Remove bold/italic markdown that might look odd
  sanitized = sanitized.replace(/\*\*([^*]+)\*\*/g, '$1')
  sanitized = sanitized.replace(/\*([^*]+)\*/g, '$1')
  sanitized = sanitized.replace(/__([^_]+)__/g, '$1')
  sanitized = sanitized.replace(/_([^_]+)_/g, '$1')

  // Remove code blocks
  sanitized = sanitized.replace(/```[\s\S]*?```/g, '')
  sanitized = sanitized.replace(/`([^`]+)`/g, '$1')

  // Remove HTML tags
  sanitized = sanitized.replace(/<[^>]+>/g, '')

  // Remove JSON blocks (sometimes AI outputs raw JSON)
  sanitized = sanitized.replace(/\{[\s\S]*?"[\s\S]*?\}/g, '')

  // Clean up extra whitespace
  sanitized = sanitized.replace(/\n{3,}/g, '\n\n')
  sanitized = sanitized.replace(/  +/g, ' ')

  return sanitized.trim()
}

/**
 * Parse agent output to extract structured response
 */
function parseAgentOutput(content: string, turns: ConversationTurn[], providerId?: string): AgentResponse {
  // Apply post-processing guardrails to sanitize the response
  const sanitizedContent = sanitizeAgentResponse(content)

  const response: AgentResponse = {
    content: sanitizedContent,
    suggestions: [],
    quickReplies: [],
    products: []
  }

  // Track which tools were used
  const toolsUsed: string[] = []

  // Extract products and cart actions from tool results
  for (const turn of turns) {
    if (turn.role === 'tool' && turn.toolResult) {
      if (turn.toolName) {
        toolsUsed.push(turn.toolName)
      }

      const result = turn.toolResult as ToolResult
      if (result.success && result.data) {
        const data = result.data as Record<string, unknown>

        // Check for cart_action (from add_to_cart tool)
        if (data.cart_action) {
          response.cartAction = data.cart_action as AgentResponse['cartAction']
        }

        // Check if it's an array of menu items
        if (Array.isArray(result.data)) {
          const items = result.data as Array<Record<string, unknown>>
          if (items.length > 0 && items[0].name_ar && items[0].price) {
            response.products = items.slice(0, 5).map(item => ({
              id: item.id as string,
              name: item.name_ar as string,
              price: item.price as number,
              image: item.image_url as string | undefined,
              hasVariants: item.has_variants as boolean | undefined,
              providerId: item.provider_id as string | undefined,
              providerName: (item.providers as { name_ar?: string })?.name_ar
            }))
          }
        }
      }
    }
  }

  // Generate dynamic quick replies based on context
  // Use provider ID from first product if available, otherwise fall back to context
  const effectiveProviderId = response.products?.[0]?.providerId || providerId

  const { suggestions, quickReplies } = generateDynamicQuickReplies(
    content,
    !!response.cartAction,
    !!(response.products && response.products.length > 0),
    response.products?.[0]?.id,
    toolsUsed,
    effectiveProviderId
  )

  response.suggestions = suggestions
  response.quickReplies = quickReplies

  return response
}

/**
 * Simple agent for quick responses (no streaming)
 */
export async function quickAgentResponse(
  userMessage: string,
  context: AgentContext
): Promise<AgentResponse> {
  return runAgent({
    context,
    messages: [{ role: 'user', content: userMessage }]
  })
}
