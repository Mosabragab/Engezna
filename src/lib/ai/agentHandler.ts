/**
 * AI Agent Handler for Engezna Smart Assistant
 *
 * This file handles the AI agent conversation loop.
 * Supports both OpenAI (GPT-4o-mini) and Anthropic (Claude 3.5 Haiku).
 *
 * Set AI_PROVIDER=claude in .env to use Claude, otherwise defaults to OpenAI.
 */

import OpenAI from 'openai'
import {
  AGENT_TOOLS,
  executeAgentTool,
  getAvailableTools,
  type ToolContext,
  type ToolResult,
  loadCustomerInsights,
  saveCustomerInsights,
  analyzeConversationForInsights
} from './agentTools'
import { validateToolParams, checkRateLimit } from './toolValidation'
import {
  buildSystemPrompt,
  type AgentContext,
  type AgentResponse,
  type ConversationTurn
} from './agentPrompt'
import { runClaudeAgentStream, runClaudeAgent } from './claudeHandler'

// =============================================================================
// AI PROVIDER CONFIGURATION
// =============================================================================

type AIProvider = 'openai' | 'claude'

function getAIProvider(): AIProvider {
  const provider = process.env.AI_PROVIDER?.toLowerCase()
  if (provider === 'claude' || provider === 'anthropic') {
    return 'claude'
  }
  return 'openai'
}

// =============================================================================
// TYPES
// =============================================================================

export interface AgentMessage {
  role: 'user' | 'assistant'
  content: string
}

export interface AgentStreamEvent {
  type: 'content' | 'tool_call' | 'tool_result' | 'done' | 'error'
  content?: string
  toolName?: string
  toolArgs?: Record<string, unknown>
  toolResult?: ToolResult
  error?: string
  response?: AgentResponse
}

export interface AgentHandlerOptions {
  context: AgentContext
  messages: AgentMessage[]
  onStream?: (event: AgentStreamEvent) => void
}

// =============================================================================
// OPENAI CLIENT (Lazy initialization)
// =============================================================================

let openaiClient: OpenAI | null = null

function getOpenAIClient(): OpenAI {
  if (!openaiClient) {
    if (!process.env.OPENAI_API_KEY) {
      throw new Error('OPENAI_API_KEY environment variable is not set')
    }
    openaiClient = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY
    })
  }
  return openaiClient
}

// =============================================================================
// CONVERT TOOLS TO OPENAI FORMAT
// =============================================================================

function convertToolsToOpenAI(context: ToolContext): OpenAI.Chat.Completions.ChatCompletionTool[] {
  const availableTools = getAvailableTools(context)

  return availableTools.map(tool => ({
    type: 'function' as const,
    function: {
      name: tool.name,
      description: tool.description,
      parameters: tool.parameters
    }
  }))
}

// =============================================================================
// MAIN AGENT HANDLER (Provider-agnostic)
// =============================================================================

export async function runAgent(options: AgentHandlerOptions): Promise<AgentResponse> {
  // Check which AI provider to use
  const provider = getAIProvider()

  if (provider === 'claude') {
    // Delegate to Claude handler
    return runClaudeAgent(options)
  }

  // OpenAI implementation below
  const { context, messages, onStream } = options

  // Load customer insights if customer is logged in
  let enrichedContext = { ...context }
  if (context.customerId) {
    try {
      const insights = await loadCustomerInsights(context.customerId)
      if (insights) {
        console.log('[runAgent] Loaded customer insights:', insights.conversation_style?.customer_type)
        enrichedContext = {
          ...context,
          customerMemory: {
            ...context.customerMemory,
            preferences: insights.preferences as { spicy?: boolean; vegetarian?: boolean; notes?: string[] },
          }
        }
      }
    } catch (error) {
      console.error('[runAgent] Failed to load customer insights:', error)
    }
  }

  // Build system prompt with enriched context
  const systemPrompt = buildSystemPrompt(enrichedContext)

  // Convert tools to OpenAI format
  const tools = convertToolsToOpenAI(context)

  // Build messages array for OpenAI
  const openaiMessages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
    { role: 'system', content: systemPrompt },
    ...messages.map(msg => ({
      role: msg.role as 'user' | 'assistant',
      content: msg.content
    }))
  ]

  // Track conversation turns for this request
  const turns: ConversationTurn[] = []
  let finalResponse: AgentResponse = { content: '' }

  // Run the agent loop (max 5 iterations to prevent infinite loops)
  for (let iteration = 0; iteration < 5; iteration++) {
    try {
      // Call OpenAI with optimized settings for natural conversation
      const completion = await getOpenAIClient().chat.completions.create({
        model: 'gpt-4o-mini', // Can upgrade to 'gpt-4o' for better conversation quality
        messages: openaiMessages,
        tools: tools.length > 0 ? tools : undefined,
        tool_choice: tools.length > 0 ? 'auto' : undefined,
        temperature: 0.85, // Higher for more natural, varied responses
        max_tokens: 1500,  // More room for detailed, helpful responses
        presence_penalty: 0.1, // Slight penalty to reduce repetition
        frequency_penalty: 0.1 // Encourage diverse vocabulary
      })

      const choice = completion.choices[0]
      const message = choice.message

      // Check if the model wants to call tools
      if (message.tool_calls && message.tool_calls.length > 0) {
        // Add assistant message with tool calls to history
        openaiMessages.push({
          role: 'assistant',
          content: message.content || '',
          tool_calls: message.tool_calls
        })

        // Execute each tool call
        for (const toolCall of message.tool_calls) {
          // Handle different tool call types
          if (toolCall.type !== 'function') continue
          const toolName = toolCall.function.name
          const toolArgs = JSON.parse(toolCall.function.arguments)

          // Stream tool call event
          onStream?.({
            type: 'tool_call',
            toolName,
            toolArgs
          })

          // Validate tool parameters before execution
          const validation = validateToolParams(toolName, toolArgs, context)
          if (!validation.valid) {
            // Return validation error as tool result
            const validationResult: ToolResult = {
              success: false,
              error: validation.error,
              message: validation.message
            }

            onStream?.({
              type: 'tool_result',
              toolName,
              toolResult: validationResult
            })

            turns.push({
              role: 'tool',
              content: JSON.stringify(validationResult),
              toolName,
              toolResult: validationResult,
              timestamp: new Date()
            })

            openaiMessages.push({
              role: 'tool',
              tool_call_id: toolCall.id,
              content: JSON.stringify(validationResult)
            })

            continue
          }

          // Check rate limits (using a simple conversation identifier)
          const conversationId = context.customerId || 'anonymous'
          const rateLimit = checkRateLimit(toolName, conversationId)
          if (!rateLimit.allowed) {
            const rateLimitResult: ToolResult = {
              success: false,
              error: 'rate_limited',
              message: rateLimit.message
            }

            onStream?.({
              type: 'tool_result',
              toolName,
              toolResult: rateLimitResult
            })

            turns.push({
              role: 'tool',
              content: JSON.stringify(rateLimitResult),
              toolName,
              toolResult: rateLimitResult,
              timestamp: new Date()
            })

            openaiMessages.push({
              role: 'tool',
              tool_call_id: toolCall.id,
              content: JSON.stringify(rateLimitResult)
            })

            continue
          }

          // Execute the tool
          const result = await executeAgentTool(toolName, toolArgs, context)

          // Stream tool result event
          onStream?.({
            type: 'tool_result',
            toolName,
            toolResult: result
          })

          // Add tool result to conversation
          turns.push({
            role: 'tool',
            content: JSON.stringify(result),
            toolName,
            toolResult: result,
            timestamp: new Date()
          })

          // Add tool result to OpenAI messages
          openaiMessages.push({
            role: 'tool',
            tool_call_id: toolCall.id,
            content: JSON.stringify(result)
          })
        }

        // Continue the loop to get the final response
        continue
      }

      // No tool calls - this is the final response
      const content = message.content || ''

      // Stream content
      onStream?.({
        type: 'content',
        content
      })

      // Parse the response to extract structured data
      finalResponse = parseAgentOutput(content, turns, context.providerId || context.cartProviderId)

      // Stream done event
      onStream?.({
        type: 'done',
        response: finalResponse
      })

      break

    } catch (error) {
      console.error('[Agent Error]:', error)

      // Never expose technical errors to users - just show a friendly message
      // Log the actual error for debugging but give user a positive experience

      finalResponse = {
        content: 'Ù…Ø´ Ù„Ø§Ù‚ÙŠ Ù†ØªØ§Ø¦Ø¬ Ø¯Ù„ÙˆÙ‚ØªÙŠ. Ø¬Ø±Ø¨ ØªØ§Ù†ÙŠ Ø£Ùˆ Ø§Ø³Ø£Ù„Ù†ÙŠ Ø³Ø¤Ø§Ù„ ØªØ§Ù†ÙŠ ğŸ˜Š',
        suggestions: ['ğŸ”„ Ø¬Ø±Ø¨ ØªØ§Ù†ÙŠ', 'ğŸ›’ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª', 'ğŸ  Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©']
      }

      onStream?.({
        type: 'done',
        response: finalResponse
      })

      break
    }
  }

  // Save customer insights after conversation (non-blocking)
  if (context.customerId && turns.length > 0) {
    const toolResults = turns
      .filter(t => t.role === 'tool' && t.toolResult)
      .map(t => ({ toolName: t.toolName || '', result: t.toolResult as ToolResult }))

    const insights = analyzeConversationForInsights(
      messages.map(m => ({ role: m.role, content: m.content })),
      toolResults
    )

    // Save insights asynchronously (don't block the response)
    saveCustomerInsights(context.customerId, insights).catch(err => {
      console.error('[runAgent] Failed to save customer insights:', err)
    })
  }

  return finalResponse
}

// =============================================================================
// STREAMING AGENT HANDLER (Provider-agnostic)
// =============================================================================

export async function* runAgentStream(options: AgentHandlerOptions): AsyncGenerator<AgentStreamEvent> {
  // Check which AI provider to use
  const provider = getAIProvider()

  if (provider === 'claude') {
    // Delegate to Claude handler
    yield* runClaudeAgentStream(options)
    return
  }

  // OpenAI implementation below
  const { context, messages } = options

  // Load customer insights if customer is logged in
  let enrichedContext = { ...context }
  if (context.customerId) {
    try {
      const insights = await loadCustomerInsights(context.customerId)
      if (insights) {
        console.log('[runAgentStream] Loaded customer insights:', insights.conversation_style?.customer_type)
        enrichedContext = {
          ...context,
          customerMemory: {
            ...context.customerMemory,
            preferences: insights.preferences as { spicy?: boolean; vegetarian?: boolean; notes?: string[] },
          }
        }
      }
    } catch (error) {
      console.error('[runAgentStream] Failed to load customer insights:', error)
    }
  }

  // Build system prompt with enriched context
  const systemPrompt = buildSystemPrompt(enrichedContext)

  // Convert tools to OpenAI format
  const tools = convertToolsToOpenAI(context)

  // Build messages array for OpenAI
  const openaiMessages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
    { role: 'system', content: systemPrompt },
    ...messages.map(msg => ({
      role: msg.role as 'user' | 'assistant',
      content: msg.content
    }))
  ]

  const turns: ConversationTurn[] = []

  // Run the agent loop
  for (let iteration = 0; iteration < 5; iteration++) {
    try {
      // Streaming with optimized settings for natural conversation
      const stream = await getOpenAIClient().chat.completions.create({
        model: 'gpt-4o-mini', // Can upgrade to 'gpt-4o' for better conversation quality
        messages: openaiMessages,
        tools: tools.length > 0 ? tools : undefined,
        tool_choice: tools.length > 0 ? 'auto' : undefined,
        temperature: 0.85, // Higher for more natural, varied responses
        max_tokens: 1500,  // More room for detailed, helpful responses
        presence_penalty: 0.1, // Slight penalty to reduce repetition
        frequency_penalty: 0.1, // Encourage diverse vocabulary
        stream: true
      })

      let accumulatedContent = ''
      const toolCalls: Array<{
        id: string
        name: string
        arguments: string
      }> = []

      for await (const chunk of stream) {
        const delta = chunk.choices[0]?.delta

        // Handle content streaming
        if (delta?.content) {
          accumulatedContent += delta.content
          yield {
            type: 'content',
            content: delta.content
          }
        }

        // Handle tool calls
        if (delta?.tool_calls) {
          for (const toolCallDelta of delta.tool_calls) {
            if (toolCallDelta.index !== undefined) {
              if (!toolCalls[toolCallDelta.index]) {
                toolCalls[toolCallDelta.index] = {
                  id: toolCallDelta.id || '',
                  name: toolCallDelta.function?.name || '',
                  arguments: ''
                }
              }

              if (toolCallDelta.id) {
                toolCalls[toolCallDelta.index].id = toolCallDelta.id
              }
              if (toolCallDelta.function?.name) {
                toolCalls[toolCallDelta.index].name = toolCallDelta.function.name
              }
              if (toolCallDelta.function?.arguments) {
                toolCalls[toolCallDelta.index].arguments += toolCallDelta.function.arguments
              }
            }
          }
        }
      }

      // Process tool calls if any
      if (toolCalls.length > 0) {
        // Add assistant message with tool calls
        openaiMessages.push({
          role: 'assistant',
          content: accumulatedContent || null,
          tool_calls: toolCalls.map(tc => ({
            id: tc.id,
            type: 'function' as const,
            function: {
              name: tc.name,
              arguments: tc.arguments
            }
          }))
        })

        // Execute each tool
        for (const toolCall of toolCalls) {
          const toolArgs = JSON.parse(toolCall.arguments)
          const toolName = toolCall.name

          yield {
            type: 'tool_call',
            toolName,
            toolArgs
          }

          // Validate tool parameters before execution
          const validation = validateToolParams(toolName, toolArgs, context)
          if (!validation.valid) {
            const validationResult: ToolResult = {
              success: false,
              error: validation.error,
              message: validation.message
            }

            yield {
              type: 'tool_result',
              toolName,
              toolResult: validationResult
            }

            turns.push({
              role: 'tool',
              content: JSON.stringify(validationResult),
              toolName,
              toolResult: validationResult,
              timestamp: new Date()
            })

            openaiMessages.push({
              role: 'tool',
              tool_call_id: toolCall.id,
              content: JSON.stringify(validationResult)
            })

            continue
          }

          // Check rate limits
          const conversationId = context.customerId || 'anonymous'
          const rateLimit = checkRateLimit(toolName, conversationId)
          if (!rateLimit.allowed) {
            const rateLimitResult: ToolResult = {
              success: false,
              error: 'rate_limited',
              message: rateLimit.message
            }

            yield {
              type: 'tool_result',
              toolName,
              toolResult: rateLimitResult
            }

            turns.push({
              role: 'tool',
              content: JSON.stringify(rateLimitResult),
              toolName,
              toolResult: rateLimitResult,
              timestamp: new Date()
            })

            openaiMessages.push({
              role: 'tool',
              tool_call_id: toolCall.id,
              content: JSON.stringify(rateLimitResult)
            })

            continue
          }

          const result = await executeAgentTool(toolName, toolArgs, context)

          yield {
            type: 'tool_result',
            toolName: toolCall.name,
            toolResult: result
          }

          turns.push({
            role: 'tool',
            content: JSON.stringify(result),
            toolName: toolCall.name,
            toolResult: result,
            timestamp: new Date()
          })

          openaiMessages.push({
            role: 'tool',
            tool_call_id: toolCall.id,
            content: JSON.stringify(result)
          })
        }

        // Continue loop
        continue
      }

      // Final response
      const finalResponse = parseAgentOutput(accumulatedContent, turns, context.providerId || context.cartProviderId)

      // Save customer insights after conversation (non-blocking)
      if (context.customerId && turns.length > 0) {
        const toolResults = turns
          .filter(t => t.role === 'tool' && t.toolResult)
          .map(t => ({ toolName: t.toolName || '', result: t.toolResult as ToolResult }))

        const insights = analyzeConversationForInsights(
          messages.map(m => ({ role: m.role, content: m.content })),
          toolResults
        )

        saveCustomerInsights(context.customerId, insights).catch(err => {
          console.error('[runAgentStream] Failed to save customer insights:', err)
        })
      }

      yield {
        type: 'done',
        response: finalResponse
      }

      return

    } catch (error) {
      console.error('[Agent Stream Error]:', error)

      // Never expose technical errors to users - just show a friendly message
      yield {
        type: 'done',
        response: {
          content: 'Ù…Ø´ Ù„Ø§Ù‚ÙŠ Ù†ØªØ§Ø¦Ø¬ Ø¯Ù„ÙˆÙ‚ØªÙŠ. Ø¬Ø±Ø¨ ØªØ§Ù†ÙŠ Ø£Ùˆ Ø§Ø³Ø£Ù„Ù†ÙŠ Ø³Ø¤Ø§Ù„ ØªØ§Ù†ÙŠ ğŸ˜Š',
          suggestions: ['ğŸ”„ Ø¬Ø±Ø¨ ØªØ§Ù†ÙŠ', 'ğŸ›’ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª', 'ğŸ  Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©']
        }
      }

      return
    }
  }

  // Max iterations reached
  yield {
    type: 'done',
    response: {
      content: 'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù…Ø´ Ù‚Ø§Ø¯Ø± Ø£ÙƒÙ…Ù„ Ø§Ù„Ø·Ù„Ø¨ Ø¯Ù„ÙˆÙ‚ØªÙŠ. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© ØªØ§Ù†ÙŠØ©.',
      suggestions: ['ğŸ”„ Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© ØªØ§Ù†ÙŠØ©']
    }
  }
}

// =============================================================================
// HELPER FUNCTIONS
// =============================================================================

/**
 * Smart Quick Reply Generator
 *
 * Analyzes the AI response content and tool results to generate
 * contextually relevant quick replies. More intelligent than hardcoded logic.
 */
function generateDynamicQuickReplies(
  content: string,
  hasCartAction: boolean,
  hasProducts: boolean,
  productId?: string,
  toolsUsed?: string[],
  providerId?: string
): { suggestions: string[]; quickReplies: AgentResponse['quickReplies'] } {

  // Helper: create products navigation payload
  const menuPayload = providerId
    ? `navigate:/ar/providers/${providerId}`
    : 'Ø¹Ø§ÙŠØ² Ø£Ø´ÙˆÙ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª'

  // Analyze content for intent signals
  const contentLower = content.toLowerCase()

  // =================================================================
  // INTENT DETECTION: Analyze what the AI said to determine best actions
  // =================================================================

  // Check if AI is asking about size/variant selection
  const isAskingVariant = contentLower.includes('Ø­Ø¬Ù…') ||
    contentLower.includes('Ø£ÙŠ Ø­Ø¬Ù…') ||
    contentLower.includes('ØµØºÙŠØ±') && contentLower.includes('ÙƒØ¨ÙŠØ±') ||
    contentLower.includes('Ø§Ø®ØªØ§Ø±')

  // Check if AI is asking about quantity
  const isAskingQuantity = contentLower.includes('ÙƒØ§Ù… ÙˆØ§Ø­Ø¯') ||
    contentLower.includes('ÙƒØ§Ù… ÙˆØ§Ø­Ø¯Ø©') ||
    contentLower.includes('Ø§Ù„ÙƒÙ…ÙŠØ©')

  // Check if AI is confirming something
  const isConfirming = contentLower.includes('ØµØ­ØŸ') ||
    contentLower.includes('ØµØ­ ÙƒØ¯Ù‡') ||
    contentLower.includes('ØªÙ…Ø§Ù… ÙƒØ¯Ù‡')

  // Check if search returned no results
  const noResults = contentLower.includes('Ù…Ø´ Ù„Ø§Ù‚ÙŠ') ||
    contentLower.includes('Ù…Ù„Ù‚ØªØ´') ||
    contentLower.includes('Ù…ÙÙŠØ´')

  // Check if AI is showing promotions
  const showingPromotions = toolsUsed?.includes('get_promotions') ||
    contentLower.includes('Ø¹Ø±Ø¶') || contentLower.includes('Ø®ØµÙ…')

  // =================================================================
  // CONTEXTUAL QUICK REPLIES (Order matters! Most decisive checks first)
  // =================================================================

  // ğŸ”´ HIGHEST PRIORITY: After adding to cart - always show cart buttons
  // This MUST come first because cart action is the most decisive signal
  if (hasCartAction) {
    return {
      suggestions: ['ğŸ›’ Ø´ÙˆÙ Ø§Ù„Ø³Ù„Ø©', 'â• Ø£Ø¶Ù Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ©', 'âœ… ÙƒÙ…Ù„ Ù„Ù„Ø¯ÙØ¹'],
      quickReplies: [
        { title: 'ğŸ›’ Ø´ÙˆÙ Ø§Ù„Ø³Ù„Ø©', payload: 'Ø§ÙŠÙ‡ ÙÙŠ Ø§Ù„Ø³Ù„Ø©ØŸ' },
        { title: 'â• Ø£Ø¶Ù Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ©', payload: 'Ø¹Ø§ÙŠØ² Ø£Ø¶ÙŠÙ Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ©' },
        { title: 'âœ… ÙƒÙ…Ù„ Ù„Ù„Ø¯ÙØ¹', payload: 'navigate:/ar/checkout' }
      ]
    }
  }

  // AI asking about provider preference (Ù…Ù† Ù…Ø·Ø¹Ù… Ù…Ø¹ÙŠÙ†ØŸ ÙˆÙ„Ø§ Ø£Ø³Ø§Ø¹Ø¯ÙƒØŸ)
  const isAskingProviderPreference =
    (contentLower.includes('Ù…Ø·Ø¹Ù… Ù…Ø¹ÙŠÙ†') || contentLower.includes('Ù…ÙƒØ§Ù† Ù…Ø¹ÙŠÙ†')) &&
    (contentLower.includes('Ø£Ø³Ø§Ø¹Ø¯Ùƒ') || contentLower.includes('Ø§Ø®ØªÙŠØ§Ø±') || contentLower.includes('ÙˆÙ„Ø§'))

  if (isAskingProviderPreference) {
    return {
      suggestions: ['ğŸ” Ø³Ø§Ø¹Ø¯Ù†ÙŠ Ø£Ø®ØªØ§Ø±', 'ğŸª Ø¹Ù†Ø¯ÙŠ Ù…Ø·Ø¹Ù… Ù…Ø¹ÙŠÙ†', 'ğŸ”¥ Ø´ÙˆÙ Ø§Ù„Ø¹Ø±ÙˆØ¶'],
      quickReplies: [
        { title: 'ğŸ” Ø³Ø§Ø¹Ø¯Ù†ÙŠ Ø£Ø®ØªØ§Ø±', payload: 'Ø³Ø§Ø¹Ø¯Ù†ÙŠ Ø£Ø®ØªØ§Ø± Ø£Ø­Ø³Ù† Ù…ÙƒØ§Ù†' },
        { title: 'ğŸª Ø¹Ù†Ø¯ÙŠ Ù…Ø·Ø¹Ù… Ù…Ø¹ÙŠÙ†', payload: 'Ø£ÙŠÙˆÙ‡ Ø¹Ù†Ø¯ÙŠ Ù…Ø·Ø¹Ù… Ù…Ø¹ÙŠÙ†' },
        { title: 'ğŸ”¥ Ø´ÙˆÙ Ø§Ù„Ø¹Ø±ÙˆØ¶', payload: 'ÙˆØ±Ù‘ÙŠÙ†ÙŠ Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø§Ù„Ø£ÙˆÙ„' }
      ]
    }
  }

  // Size/Variant selection needed
  // Only show size buttons if the content explicitly mentions these standard sizes
  // Don't show for other variants like "Ø¹Ø§Ø¯ÙŠ/Ø³ÙˆØ¨Ø±" or "Ø±Ø¨Ø¹ ÙƒÙŠÙ„Ùˆ/Ù†Øµ ÙƒÙŠÙ„Ùˆ"
  const hasStandardSizes = contentLower.includes('ØµØºÙŠØ±') &&
    contentLower.includes('ÙˆØ³Ø·') &&
    contentLower.includes('ÙƒØ¨ÙŠØ±')

  if (isAskingVariant && hasProducts && hasStandardSizes) {
    return {
      suggestions: ['ØµØºÙŠØ±', 'ÙˆØ³Ø·', 'ÙƒØ¨ÙŠØ±'],
      quickReplies: [
        { title: 'ğŸ“ ØµØºÙŠØ±', payload: 'Ø¹Ø§ÙŠØ² Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ØµØºÙŠØ±' },
        { title: 'ğŸ“ ÙˆØ³Ø·', payload: 'Ø¹Ø§ÙŠØ² Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ÙˆØ³Ø·' },
        { title: 'ğŸ“ ÙƒØ¨ÙŠØ±', payload: 'Ø¹Ø§ÙŠØ² Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ÙƒØ¨ÙŠØ±' }
      ]
    }
  }

  // For other variant types (Ø¹Ø§Ø¯ÙŠ/Ø³ÙˆØ¨Ø±, Ø±Ø¨Ø¹/Ù†Øµ ÙƒÙŠÙ„Ùˆ), show generic add button
  if (isAskingVariant && hasProducts && !hasStandardSizes) {
    return {
      suggestions: ['Ø¶ÙŠÙ Ù„Ù„Ø³Ù„Ø©', 'ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØªØ±'],
      quickReplies: [
        { title: 'âœ… Ø¶ÙŠÙ Ù„Ù„Ø³Ù„Ø©', payload: 'Ø¶ÙŠÙÙ‡ Ù„Ù„Ø³Ù„Ø©' },
        { title: 'ğŸ“‹ ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØªØ±', payload: 'Ø¹Ø§ÙŠØ² ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØªØ±' },
        { title: 'ğŸ” Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ©', payload: 'Ø¹Ø§ÙŠØ² Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ©' }
      ]
    }
  }

  // Quantity selection needed
  if (isAskingQuantity) {
    return {
      suggestions: ['1ï¸âƒ£ ÙˆØ§Ø­Ø¯Ø©', '2ï¸âƒ£ Ø§ØªÙ†ÙŠÙ†', '3ï¸âƒ£ ØªÙ„Ø§ØªØ©'],
      quickReplies: [
        { title: '1ï¸âƒ£ ÙˆØ§Ø­Ø¯Ø©', payload: 'ÙˆØ§Ø­Ø¯Ø© Ø¨Ø³' },
        { title: '2ï¸âƒ£ Ø§ØªÙ†ÙŠÙ†', payload: 'Ø§ØªÙ†ÙŠÙ†' },
        { title: '3ï¸âƒ£ ØªÙ„Ø§ØªØ©', payload: 'ØªÙ„Ø§ØªØ©' }
      ]
    }
  }

  // Confirmation needed
  if (isConfirming) {
    return {
      suggestions: ['âœ… Ø£ÙŠÙˆÙ‡ ØªÙ…Ø§Ù…', 'âŒ Ù„Ø£ ØºÙŠØ±', 'ğŸ”„ Ø¹Ø¯Ù„ Ø§Ù„ÙƒÙ…ÙŠØ©'],
      quickReplies: [
        { title: 'âœ… Ø£ÙŠÙˆÙ‡ ØªÙ…Ø§Ù…', payload: 'Ø£ÙŠÙˆÙ‡ Ø¶ÙŠÙ Ù„Ù„Ø³Ù„Ø©' },
        { title: 'âŒ Ù„Ø£ ØºÙŠØ±', payload: 'Ù„Ø£ Ø¹Ø§ÙŠØ² Ø£ØºÙŠØ±' },
        { title: 'ğŸ”„ Ø¹Ø¯Ù„ Ø§Ù„ÙƒÙ…ÙŠØ©', payload: 'Ø¹Ø§ÙŠØ² Ø£ØºÙŠØ± Ø§Ù„ÙƒÙ…ÙŠØ©' }
      ]
    }
  }

  // Provider selection/disambiguation - when asking user to choose a provider
  const isProviderSelection = contentLower.includes('ØªÙØ¶Ù„ ØªØ·Ù„Ø¨ Ù…Ù† Ù…ÙŠÙ†') ||
    contentLower.includes('Ø§Ø®ØªØ§Ø± Ø§Ù„Ù…Ø·Ø¹Ù…') ||
    contentLower.includes('ØªÙØ¶Ù„ Ù…ÙŠÙ†') ||
    (contentLower.includes('Ù„Ù‚ÙŠØª') && contentLower.includes('Ù…ÙƒØ§Ù†'))

  if (isProviderSelection) {
    return {
      suggestions: ['ğŸ† Ø§Ù„Ø£Ø¹Ù„Ù‰ ØªÙ‚ÙŠÙŠÙ…Ø§Ù‹', 'ğŸ’° Ø§Ù„Ø£Ø±Ø®Øµ', 'ğŸ”¥ Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯Ù‡ Ø¹Ø±ÙˆØ¶'],
      quickReplies: [
        { title: 'ğŸ† Ø§Ù„Ø£Ø¹Ù„Ù‰ ØªÙ‚ÙŠÙŠÙ…Ø§Ù‹', payload: 'Ø¹Ø§ÙŠØ² Ø§Ù„Ø£Ø¹Ù„Ù‰ ØªÙ‚ÙŠÙŠÙ…Ø§Ù‹' },
        { title: 'ğŸ’° Ø§Ù„Ø£Ø±Ø®Øµ', payload: 'Ø¹Ø§ÙŠØ² Ø§Ù„Ø£Ø±Ø®Øµ' },
        { title: 'ğŸ”¥ Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯Ù‡ Ø¹Ø±ÙˆØ¶', payload: 'Ø¹Ø§ÙŠØ² Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯Ù‡ Ø¹Ø±ÙˆØ¶' },
        { title: 'ğŸ” Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ©', payload: 'Ø¹Ø§ÙŠØ² Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ© Ø®Ø§Ù„Øµ' }
      ]
    }
  }

  // No results found - help user search differently
  if (noResults) {
    return {
      suggestions: ['ğŸ” Ø¨Ø­Ø« ØªØ§Ù†ÙŠ', 'ğŸ›’ Ø´ÙˆÙ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª', 'ğŸ”¥ Ø§Ù„Ø¹Ø±ÙˆØ¶'],
      quickReplies: [
        { title: 'ğŸ” Ø¨Ø­Ø« ØªØ§Ù†ÙŠ', payload: 'Ø¹Ø§ÙŠØ² Ø£Ø¨Ø­Ø« Ø¹Ù† Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ©' },
        { title: 'ğŸ›’ Ø´ÙˆÙ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª', payload: menuPayload },
        { title: 'ğŸ”¥ Ø§Ù„Ø¹Ø±ÙˆØ¶', payload: 'ÙÙŠÙ‡ Ø¹Ø±ÙˆØ¶ Ø§ÙŠÙ‡ Ø¯Ù„ÙˆÙ‚ØªÙŠØŸ' }
      ]
    }
  }

  // After search with products found
  if (hasProducts && productId) {
    return {
      suggestions: ['âœ… Ø¶ÙŠÙ Ù„Ù„Ø³Ù„Ø©', 'ğŸ“‹ ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØªØ±', 'ğŸ” Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ©'],
      quickReplies: [
        { title: 'âœ… Ø¶ÙŠÙ Ù„Ù„Ø³Ù„Ø©', payload: 'Ø¶ÙŠÙ Ø§Ù„Ø£ÙˆÙ„ Ù„Ù„Ø³Ù„Ø©' },
        { title: 'ğŸ“‹ ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØªØ±', payload: 'Ø¹Ø§ÙŠØ² ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØªØ±' },
        { title: 'ğŸ” Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ©', payload: 'Ø¹Ø§ÙŠØ² Ø£Ø¨Ø­Ø« Ø¹Ù† Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ©' }
      ]
    }
  }

  // Showing promotions
  if (showingPromotions) {
    return {
      suggestions: ['ğŸ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¹Ø±Ø¶', 'ğŸ›’ Ø´ÙˆÙ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª', 'ğŸ” Ø¨Ø­Ø«'],
      quickReplies: [
        { title: 'ğŸ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¹Ø±Ø¶', payload: 'Ø¹Ø§ÙŠØ² Ø£Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¹Ø±Ø¶ Ø¯Ù‡' },
        { title: 'ğŸ›’ Ø´ÙˆÙ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª', payload: menuPayload },
        { title: 'ğŸ” Ø¨Ø­Ø«', payload: 'Ø¹Ø§ÙŠØ² Ø£Ø¨Ø­Ø« Ø¹Ù† Ø­Ø§Ø¬Ø©' }
      ]
    }
  }

  // Order tracking context
  if (toolsUsed?.includes('track_order') || toolsUsed?.includes('get_order_status')) {
    return {
      suggestions: ['ğŸ“ ØªØªØ¨Ø¹ Ø§Ù„Ø·Ù„Ø¨', 'ğŸ“ Ø§ØªØµÙ„ Ø¨Ø§Ù„Ù…Ø·Ø¹Ù…', 'âŒ Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø·Ù„Ø¨'],
      quickReplies: [
        { title: 'ğŸ“ ØªØªØ¨Ø¹ Ø§Ù„Ø·Ù„Ø¨', payload: 'ÙÙŠÙ† Ø·Ù„Ø¨ÙŠ Ø¯Ù„ÙˆÙ‚ØªÙŠØŸ' },
        { title: 'ğŸ“ Ø§ØªØµÙ„ Ø¨Ø§Ù„Ù…Ø·Ø¹Ù…', payload: 'Ø¹Ø§ÙŠØ² Ø±Ù‚Ù… Ø§Ù„Ù…Ø·Ø¹Ù…' },
        { title: 'âŒ Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø·Ù„Ø¨', payload: 'Ø¹Ø§ÙŠØ² Ø£Ù„ØºÙŠ Ø§Ù„Ø·Ù„Ø¨' }
      ]
    }
  }

  // Complaint or problem context
  if (contentLower.includes('Ù…Ø´ÙƒÙ„Ø©') || contentLower.includes('Ø´ÙƒÙˆÙ‰') ||
      contentLower.includes('Ø²Ø¹Ù„Ø§Ù†') || contentLower.includes('Ù…Ø¹Ù„Ø´')) {
    return {
      suggestions: ['ğŸ“ ÙƒÙ„Ù… Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡', 'ğŸ“ Ø§ÙƒØªØ¨ Ø´ÙƒÙˆÙ‰', 'ğŸ”™ Ø±Ø¬ÙˆØ¹'],
      quickReplies: [
        { title: 'ğŸ“ ÙƒÙ„Ù… Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡', payload: 'Ø¹Ø§ÙŠØ² Ø£ÙƒÙ„Ù… Ø­Ø¯ Ù…Ù† Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡' },
        { title: 'ğŸ“ Ø§ÙƒØªØ¨ Ø´ÙƒÙˆÙ‰', payload: 'Ø¹Ø§ÙŠØ² Ø£Ø¹Ù…Ù„ Ø´ÙƒÙˆÙ‰ Ø±Ø³Ù…ÙŠØ©' },
        { title: 'ğŸ”™ Ø±Ø¬ÙˆØ¹', payload: 'Ø®Ù„Ø§Øµ Ù…Ø´ Ù…Ø­ØªØ§Ø¬' }
      ]
    }
  }

  // Cart summary context
  if (toolsUsed?.includes('get_cart_summary') ||
      (contentLower.includes('Ø§Ù„Ø³Ù„Ø©') && contentLower.includes('ÙÙŠÙ‡Ø§'))) {
    return {
      suggestions: ['âœ… ÙƒÙ…Ù„ Ù„Ù„Ø¯ÙØ¹', 'â• Ø£Ø¶Ù Ø­Ø§Ø¬Ø©', 'ğŸ—‘ï¸ ÙØ¶ÙŠ Ø§Ù„Ø³Ù„Ø©'],
      quickReplies: [
        { title: 'âœ… ÙƒÙ…Ù„ Ù„Ù„Ø¯ÙØ¹', payload: 'navigate:/ar/checkout' },
        { title: 'â• Ø£Ø¶Ù Ø­Ø§Ø¬Ø©', payload: 'Ø¹Ø§ÙŠØ² Ø£Ø¶ÙŠÙ Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ©' },
        { title: 'ğŸ—‘ï¸ ÙØ¶ÙŠ Ø§Ù„Ø³Ù„Ø©', payload: 'Ø§Ù…Ø³Ø­ Ø§Ù„Ø³Ù„Ø© ÙƒÙ„Ù‡Ø§' }
      ]
    }
  }

  // Delivery info context - show products button instead of "ØªÙ…Ø§Ù… Ø§Ø·Ù„Ø¨"
  if (toolsUsed?.includes('get_delivery_info') ||
      contentLower.includes('ØªÙˆØµÙŠÙ„') || contentLower.includes('Ø±Ø³ÙˆÙ…')) {
    return {
      suggestions: ['ğŸ›’ Ø´ÙˆÙ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª', 'ğŸ” Ø¨Ø­Ø« ØªØ§Ù†ÙŠ', 'ğŸ”¥ Ø§Ù„Ø¹Ø±ÙˆØ¶'],
      quickReplies: [
        { title: 'ğŸ›’ Ø´ÙˆÙ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª', payload: menuPayload },
        { title: 'ğŸ” Ø¨Ø­Ø« ØªØ§Ù†ÙŠ', payload: 'Ø¹Ø§ÙŠØ² Ø£Ø¨Ø­Ø« Ø¹Ù† Ø­Ø§Ø¬Ø©' },
        { title: 'ğŸ”¥ Ø§Ù„Ø¹Ø±ÙˆØ¶', payload: 'ÙÙŠÙ‡ Ø¹Ø±ÙˆØ¶ Ø§ÙŠÙ‡ØŸ' }
      ]
    }
  }

  // Menu/categories context
  if (toolsUsed?.includes('get_provider_categories') || toolsUsed?.includes('get_menu_items')) {
    return {
      suggestions: ['ğŸ• Ø¨ÙŠØªØ²Ø§', 'ğŸ” Ø¨Ø±Ø¬Ø±', 'ğŸ¥— Ø³Ù„Ø·Ø§Øª', 'ğŸ” Ø¨Ø­Ø«'],
      quickReplies: [
        { title: 'ğŸ• Ø¨ÙŠØªØ²Ø§', payload: 'Ø¹Ø§ÙŠØ² Ø¨ÙŠØªØ²Ø§' },
        { title: 'ğŸ” Ø¨Ø±Ø¬Ø±', payload: 'Ø¹Ø§ÙŠØ² Ø¨Ø±Ø¬Ø±' },
        { title: 'ğŸ¥— Ø³Ù„Ø·Ø§Øª', payload: 'Ø¹Ø§ÙŠØ² Ø³Ù„Ø·Ø©' },
        { title: 'ğŸ” Ø¨Ø­Ø«', payload: 'Ø¹Ø§ÙŠØ² Ø£Ø¨Ø­Ø« Ø¹Ù† Ø­Ø§Ø¬Ø© Ù…Ø¹ÙŠÙ†Ø©' }
      ]
    }
  }

  // Greeting/welcome context - guide to provider selection
  if (contentLower.includes('Ø£Ù‡Ù„Ø§Ù‹') || contentLower.includes('Ø£Ù‡Ù„Ø§') ||
      contentLower.includes('ØµØ¨Ø§Ø­') || contentLower.includes('Ù…Ø³Ø§Ø¡') ||
      contentLower.includes('Ø¹Ø§ÙŠØ² ØªØ·Ù„Ø¨ Ù…Ù†ÙŠÙ†') || contentLower.includes('Ø¹Ø§ÙŠØ²Ø© ØªØ·Ù„Ø¨ÙŠ Ù…Ù†ÙŠÙ†')) {
    return {
      suggestions: ['ğŸª Ø¹Ù†Ø¯ÙŠ Ù…ÙƒØ§Ù† Ù…Ø¹ÙŠÙ†', 'ğŸ” Ø³Ø§Ø¹Ø¯Ù†ÙŠ Ø£Ø®ØªØ§Ø±', 'ğŸ”¥ Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯Ù‡Ù… Ø¹Ø±ÙˆØ¶'],
      quickReplies: [
        { title: 'ğŸª Ø¹Ù†Ø¯ÙŠ Ù…ÙƒØ§Ù† Ù…Ø¹ÙŠÙ†', payload: 'Ø¹Ø§ÙŠØ² Ø£Ø·Ù„Ø¨ Ù…Ù† Ù…ÙƒØ§Ù† Ù…Ø¹ÙŠÙ†' },
        { title: 'ğŸ” Ø³Ø§Ø¹Ø¯Ù†ÙŠ Ø£Ø®ØªØ§Ø±', payload: 'Ø³Ø§Ø¹Ø¯Ù†ÙŠ Ø£Ø®ØªØ§Ø± Ù…ÙƒØ§Ù†' },
        { title: 'ğŸ”¥ Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯Ù‡Ù… Ø¹Ø±ÙˆØ¶', payload: 'ÙˆØ±Ù‘ÙŠÙ†ÙŠ Ø§Ù„Ø£Ù…Ø§ÙƒÙ† Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯Ù‡Ø§ Ø¹Ø±ÙˆØ¶' }
      ]
    }
  }

  // Default suggestions - context-aware
  // If user has selected a provider, show products button; otherwise guide to provider selection
  if (providerId) {
    return {
      suggestions: ['ğŸ›’ Ø´ÙˆÙ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª', 'ğŸ”¥ Ø§Ù„Ø¹Ø±ÙˆØ¶', 'ğŸ“¦ Ø·Ù„Ø¨Ø§ØªÙŠ'],
      quickReplies: [
        { title: 'ğŸ›’ Ø´ÙˆÙ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª', payload: menuPayload },
        { title: 'ğŸ”¥ Ø§Ù„Ø¹Ø±ÙˆØ¶', payload: 'ÙÙŠÙ‡ Ø¹Ø±ÙˆØ¶ Ø§ÙŠÙ‡ØŸ' },
        { title: 'ğŸ“¦ Ø·Ù„Ø¨Ø§ØªÙŠ', payload: 'ÙÙŠÙ† Ø·Ù„Ø¨Ø§ØªÙŠØŸ' }
      ]
    }
  }

  // No provider selected - guide to selection
  return {
    suggestions: ['ğŸª Ø¹Ù†Ø¯ÙŠ Ù…ÙƒØ§Ù† Ù…Ø¹ÙŠÙ†', 'ğŸ” Ø³Ø§Ø¹Ø¯Ù†ÙŠ Ø£Ø®ØªØ§Ø±', 'ğŸ”¥ Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯Ù‡Ù… Ø¹Ø±ÙˆØ¶'],
    quickReplies: [
      { title: 'ğŸª Ø¹Ù†Ø¯ÙŠ Ù…ÙƒØ§Ù† Ù…Ø¹ÙŠÙ†', payload: 'Ø¹Ø§ÙŠØ² Ø£Ø·Ù„Ø¨ Ù…Ù† Ù…ÙƒØ§Ù† Ù…Ø¹ÙŠÙ†' },
      { title: 'ğŸ” Ø³Ø§Ø¹Ø¯Ù†ÙŠ Ø£Ø®ØªØ§Ø±', payload: 'Ø³Ø§Ø¹Ø¯Ù†ÙŠ Ø£Ø®ØªØ§Ø± Ù…ÙƒØ§Ù†' },
      { title: 'ğŸ”¥ Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯Ù‡Ù… Ø¹Ø±ÙˆØ¶', payload: 'ÙˆØ±Ù‘ÙŠÙ†ÙŠ Ø§Ù„Ø£Ù…Ø§ÙƒÙ† Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯Ù‡Ø§ Ø¹Ø±ÙˆØ¶' }
    ]
  }
}

/**
 * Sanitize AI response to remove unsafe content
 * This is a POST-PROCESSING GUARDRAIL to ensure no URLs or markdown images slip through
 */
function sanitizeAgentResponse(content: string): string {
  let sanitized = content

  // Remove markdown image syntax: ![alt](url)
  sanitized = sanitized.replace(/!\[([^\]]*)\]\([^)]+\)/g, '')

  // Remove markdown links but keep the text: [text](url) -> text
  sanitized = sanitized.replace(/\[([^\]]+)\]\([^)]+\)/g, '$1')

  // Remove raw URLs (http, https, ftp)
  sanitized = sanitized.replace(/https?:\/\/[^\s<>"\)]+/gi, '')
  sanitized = sanitized.replace(/ftp:\/\/[^\s<>"\)]+/gi, '')

  // Remove any remaining URL-like patterns
  sanitized = sanitized.replace(/www\.[^\s<>"\)]+/gi, '')

  // Remove bold/italic markdown that might look odd
  sanitized = sanitized.replace(/\*\*([^*]+)\*\*/g, '$1')
  sanitized = sanitized.replace(/\*([^*]+)\*/g, '$1')
  sanitized = sanitized.replace(/__([^_]+)__/g, '$1')
  sanitized = sanitized.replace(/_([^_]+)_/g, '$1')

  // Remove code blocks
  sanitized = sanitized.replace(/```[\s\S]*?```/g, '')
  sanitized = sanitized.replace(/`([^`]+)`/g, '$1')

  // Remove HTML tags
  sanitized = sanitized.replace(/<[^>]+>/g, '')

  // Remove JSON blocks (sometimes AI outputs raw JSON)
  sanitized = sanitized.replace(/\{[\s\S]*?"[\s\S]*?\}/g, '')

  // Clean up extra whitespace
  sanitized = sanitized.replace(/\n{3,}/g, '\n\n')
  sanitized = sanitized.replace(/  +/g, ' ')

  return sanitized.trim()
}

/**
 * Parse agent output to extract structured response
 */
function parseAgentOutput(content: string, turns: ConversationTurn[], providerId?: string): AgentResponse {
  // Apply post-processing guardrails to sanitize the response
  const sanitizedContent = sanitizeAgentResponse(content)

  const response: AgentResponse = {
    content: sanitizedContent,
    suggestions: [],
    quickReplies: [],
    products: [],
    cartActions: []  // Collect ALL cart actions from multiple tool calls
  }

  // Track which tools were used
  const toolsUsed: string[] = []

  // Extract products and cart actions from tool results
  for (const turn of turns) {
    if (turn.role === 'tool' && turn.toolResult) {
      if (turn.toolName) {
        toolsUsed.push(turn.toolName)
      }

      const result = turn.toolResult as ToolResult

      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // FIX: Extract discovered_provider_id from tool results
      // Check BOTH root level (lookup_provider) AND inside data (search_menu)
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      if (result.discovered_provider_id && !response.discoveredProviderId) {
        response.discoveredProviderId = result.discovered_provider_id
        response.discoveredProviderName = result.discovered_provider_name
        console.log('[parseAgentOutput] Discovered provider (root):', result.discovered_provider_id, result.discovered_provider_name)
      }

      if (result.success && result.data) {
        const data = result.data as Record<string, unknown>

        // Also check inside data (for search_menu)
        if (data.discovered_provider_id && !response.discoveredProviderId) {
          response.discoveredProviderId = data.discovered_provider_id as string
          response.discoveredProviderName = data.discovered_provider_name as string | undefined
          console.log('[parseAgentOutput] Discovered provider (data):', data.discovered_provider_id, data.discovered_provider_name)
        }

        // Check for cart_action (from add_to_cart tool)
        // Collect ALL cart actions instead of overwriting
        if (data.cart_action) {
          const cartAction = data.cart_action as AgentResponse['cartAction']
          response.cartActions!.push(cartAction!)
          // Also set single cartAction for backward compatibility (last one)
          response.cartAction = cartAction
        }

        // Check if it's an array of menu items
        if (Array.isArray(result.data)) {
          const items = result.data as Array<Record<string, unknown>>
          if (items.length > 0 && items[0].name_ar && items[0].price) {
            response.products = items.slice(0, 5).map(item => ({
              id: item.id as string,
              name: item.name_ar as string,
              price: item.price as number,
              image: item.image_url as string | undefined,
              hasVariants: item.has_variants as boolean | undefined,
              providerId: item.provider_id as string | undefined,
              providerName: (item.providers as { name_ar?: string })?.name_ar
            }))

            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            // FIX: Store pending item in sessionMemory for next request
            // This allows the AI to remember the item IDs when user says "Ø¶ÙŠÙ"
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            const firstItem = items[0]
            const variants = firstItem.variants as Array<{ id: string; name_ar: string; price: number }> | undefined

            response.sessionMemory = {
              pending_item: {
                id: firstItem.id as string,
                name_ar: firstItem.name_ar as string,
                price: firstItem.price as number,
                provider_id: firstItem.provider_id as string,
                provider_name_ar: (firstItem.providers as { name_ar?: string })?.name_ar,
                has_variants: firstItem.has_variants as boolean | undefined,
                variants: variants?.map(v => ({
                  id: v.id,
                  name_ar: v.name_ar,
                  price: v.price
                }))
              }
            }
            console.log('[parseAgentOutput] Stored pending item:', response.sessionMemory.pending_item?.name_ar, 'with', variants?.length || 0, 'variants')
          }
        }
      }
    }
  }

  // Generate dynamic quick replies based on context
  // Use provider ID from first product if available, otherwise fall back to context
  const effectiveProviderId = response.products?.[0]?.providerId || providerId

  // Check for cart actions (both singular and plural)
  const hasAnyCartAction = !!(response.cartAction || (response.cartActions && response.cartActions.length > 0))

  const { suggestions, quickReplies } = generateDynamicQuickReplies(
    content,
    hasAnyCartAction,
    !!(response.products && response.products.length > 0),
    response.products?.[0]?.id,
    toolsUsed,
    effectiveProviderId
  )

  response.suggestions = suggestions
  response.quickReplies = quickReplies

  return response
}

/**
 * Simple agent for quick responses (no streaming)
 */
export async function quickAgentResponse(
  userMessage: string,
  context: AgentContext
): Promise<AgentResponse> {
  return runAgent({
    context,
    messages: [{ role: 'user', content: userMessage }]
  })
}
